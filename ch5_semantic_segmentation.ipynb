{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch5_semantic_segmentation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/deep_learning_for_computer_vision/blob/master/ch5_semantic_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_XnODIf5Au4K",
        "colab_type": "code",
        "outputId": "1fb4090f-6f0d-45ff-87f5-4e2955e4f851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9NLZBit2BSG7",
        "colab_type": "code",
        "outputId": "61396e59-f614-4c05-960c-a03189dab693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My' 'Drive/Colab' 'Notebooks/Deep_Learning_for_Computer_Vision/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Deep_Learning_for_Computer_Vision\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Psi5EkwiNNFq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OWKsuJnHzDwX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Predict Pixels\n",
        "Image classification is the task of predicting labels or categories. \n",
        "\n",
        "Object detection is the task of predicting a list of several deep learning-based algorithms\n",
        "with its corresponding bounding box.\n",
        "\n",
        "**Semantic segmentation** is the task of predicting pixel-wise labels.\n",
        "\n",
        "**Instance segmentation** is the task of segmenting every instance with a pixel-wise label. Instance segmentation can be thought of as an extension of object\n",
        "detection with pixel-level labels."
      ]
    },
    {
      "metadata": {
        "id": "0rC5n4110IJw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Datasets\n",
        "The `PASCAL` and `COCO` datasets can be used for the segmentation task as well. The annotations are different as\n",
        "they are labelled pixel-wise. New algorithms are usually benchmarked against\n",
        "the `COCO` dataset. `COCO` also has stuff datasets such as grass, wall, and sky. The pixel\n",
        "accuracy property can be used as a metric for evaluating algorithms.\n",
        "\n",
        "Other datasets:\n",
        "* http://www.cs.bu.edu/~betke/BiomedicalImageSegmentation\n",
        "* https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening/data\n",
        "* https://www.kaggle.com/c/diabetic-retinopathy-detection\n",
        "* https://grand-challenge.org/all_challenges\n",
        "* http://www.via.cornell.edu/databases\n",
        "* https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection\n",
        "* https://aws.amazon.com/public-datasets/spacenet\n",
        "* https://www.iarpa.gov/challenges/fmow.html\n",
        "* https://www.kaggle.com/c/planet-understanding-the-amazon-from-space"
      ]
    },
    {
      "metadata": {
        "id": "Q14UxeXa0s94",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Algorithms for Semantic Segmentation\n",
        "A sliding window approach can be applied at a pixel\n",
        "level for segmentation. A sliding window approach takes an image and breaks\n",
        "the image into smaller crops. Every crop of the image is classified for a label.\n",
        "This approach is expensive and inefficient because it doesn't reuse the shared\n",
        "features between the overlapping patches."
      ]
    },
    {
      "metadata": {
        "id": "BEc-CWI904kv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Fully Convolutional Network (FCN)\n",
        "The **Fully Convolutional Network (FCN)** introduced the idea of an end-to-end convolutional network.\n",
        "\n",
        "Any standard CNN architecture can be used for FCN by removing the fully connected layers. The fully connected layers are replaced by\n",
        "a convolution layer. The depth is higher in the final layers and the size is smaller.\n",
        "Hence, 1D convolution can be performed to reach the desired number of labels.\n",
        "But for segmentation, the spatial dimension has to be preserved. Hence, the full\n",
        "convolution network is constructed without a max pooling, as shown below:\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/fcn.JPG?raw=true)\n",
        "\n",
        "The loss for this network is computed by averaging the cross-entropy loss of\n",
        "every pixel and mini-batch.\n",
        "\n",
        "The final layer has a depth equal to the number of\n",
        "classes. FCN is similar to object detection except that the spatial dimension is\n",
        "preserved. The output produced by the architecture will be coarse as some pixels\n",
        "may be mispredicted."
      ]
    },
    {
      "metadata": {
        "id": "fRyDFKzDGiau",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##The SegNet Architecture\n",
        "The **SegNet** has an encoder and decoder approach. The encoder has various\n",
        "convolution layers and decoder has various deconvolution layers. SegNet\n",
        "improved the coarse outputs produced by FCN. Because of this, it is less\n",
        "intensive on memory. When the features are reduced in dimensions, it is\n",
        "upsampled again to the image size by deconvolution, reversing the convolution\n",
        "effects. Deconvolution learns the parameters for upsampling. The output of such\n",
        "architecture will be coarse due to the loss of information in pooling layers.\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/segnet.JPG?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "vOK-x0GzHCua",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Upsample the layers by pooling\n",
        "Max pooling is a sampling strategy that picks the maximum value from a window.\n",
        "\n",
        "The reverse process is for upsampling. Now each value can be surrounded with zeros to upsample the layer:\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/upsample1.JPG?raw=true)\n",
        "\n",
        "Another way to add zeros is to remember the locations of downsampling and use it for upsampling:\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/upsample2.JPG?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "eGdgEQh-Iu-x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Sample the layers by convolution\n",
        "The layers can be upsampled or downsampled directly using convolution. The\n",
        "stride used for convolution can be increased to cause downsampling:\n",
        "\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/downsample.JPG?raw=true)\n",
        "\n",
        "Downsampling by convolution is called **atrous convolution** or **dilated\n",
        "convolution** or **strided convolution**.\n",
        "\n",
        "Similarly, it can be reversed to upsample\n",
        "by learning a kernel:\n",
        "\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/upsample.JPG?raw=true)\n",
        "\n",
        "Upsampling directly using a convolution can be termed as **transposed\n",
        "convolution**, **deconvolution** or **fractionally strided\n",
        "convolution** or **up-convolution**."
      ]
    },
    {
      "metadata": {
        "id": "TDZFIqEGNGJk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Build SegNet using TensorFlow"
      ]
    },
    {
      "metadata": {
        "id": "gxDj216dO3S4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rPa7b_xTNJq0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_height = 360\n",
        "input_width = 480\n",
        "kernel = 3\n",
        "filter_size = 64\n",
        "pad = 1\n",
        "pool_size = 2\n",
        "nClasses = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5fBTKuifNbAz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Layer(input_shape=(input_height, input_width, 3)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wP8wyOZpNlkD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "model.add(tf.keras.layers.ZeroPadding2D(padding=(pad, pad)))\n",
        "model.add(tf.keras.layers.Conv2D(filter_size, \n",
        "                                 kernel, \n",
        "                                 kernel,\n",
        "                                 padding='valid'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Activation('relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
        "\n",
        "model.add(tf.keras.layers.ZeroPadding2D(padding=(pad, pad)))\n",
        "model.add(tf.keras.layers.Conv2D(128, \n",
        "                                 kernel, \n",
        "                                 kernel, \n",
        "                                 padding='valid'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Activation('relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
        "\n",
        "model.add(tf.keras.layers.ZeroPadding2D(padding=(pad, pad)))\n",
        "model.add(tf.keras.layers.Conv2D(256, \n",
        "                                 kernel, \n",
        "                                 kernel, \n",
        "                                 padding='valid'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Activation('relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
        "\n",
        "model.add(tf.keras.layers.ZeroPadding2D(padding=(pad, pad)))\n",
        "model.add(tf.keras.layers.Conv2D(512, \n",
        "                                 kernel, \n",
        "                                 kernel, \n",
        "                                 padding='valid'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Activation('relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aF-BK1tDQiCw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Decoder\n",
        "model.add(tf.keras.layers.ZeroPadding2D(padding=(pad, pad)))\n",
        "model.add(tf.keras.layers.Conv2D(512, \n",
        "                                 kernel, \n",
        "                                 kernel, \n",
        "                                 padding='valid'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.UpSampling2D(size=(pool_size, pool_size)))\n",
        "model.add(tf.keras.layers.ZeroPadding2D(padding=(pad, pad)))\n",
        "model.add(tf.keras.layers.Conv2D(256, \n",
        "                                 kernel, \n",
        "                                 kernel, \n",
        "                                 padding='valid'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.UpSampling2D(size=(pool_size, pool_size)))\n",
        "model.add(tf.keras.layers.ZeroPadding2D(padding=(pad, pad)))\n",
        "model.add(tf.keras.layers.Conv2D(128, \n",
        "                                 kernel, \n",
        "                                 kernel, \n",
        "                                 padding='valid'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.UpSampling2D(size=(pool_size, pool_size)))\n",
        "model.add(tf.keras.layers.ZeroPadding2D(padding=(pad, pad)))\n",
        "model.add(tf.keras.layers.Conv2D(filter_size, \n",
        "                                 kernel, \n",
        "                                 kernel, \n",
        "                                 padding='valid'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(nClasses, 1, 1, border_mode='valid', ))\n",
        "\n",
        "model.outputHeight = model.output_shape[-2]\n",
        "model.outputWidth = model.output_shape[-1]\n",
        "\n",
        "model.add(tf.keras.layers.Reshape((nClasses, model.output_shape[-2] * model.output_shape[-1]),\n",
        "                                  input_shape=(nClasses, \n",
        "                                               model.output_shape[-2], \n",
        "                                               model.output_shape[-1])))\n",
        "\n",
        "model.add(tf.keras.layers.Permute((2, 1)))\n",
        "model.add(tf.keras.layers.Activation('softmax'))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vdDVopSPRAhs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Skipping Connections\n",
        "The coarseness of segmentation output can be limited by skip architecture, and\n",
        "higher resolutions can be obtained.\n",
        "\n",
        "Another alternative way is to scale up the last\n",
        "three layers and average them as shown below:\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/skip_connection.JPG?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "OD4CktnBZvYv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Dilated Convolutions\n",
        "The pixel-wise classification and image classification are structurally different.\n",
        "\n",
        "The pooling layers that decrease information will produce coarse segmentation. \n",
        "\n",
        "Pooling is essential to have a wider view and allows sampling. The **dilated convolution** is used to solve\n",
        "this problem for less-lossy sampling while having a wider view. \n",
        "\n",
        "The dilated convolution is essentially convolution by skipping every pixel in the window as shown below:\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/dilated_conv.JPG?raw=true)\n",
        "\n",
        "The dilation distance varies from layer to layer. The output of such a\n",
        "segmentation result is upscaled for a finer resolution."
      ]
    },
    {
      "metadata": {
        "id": "OsNipJrtaogN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##DeepLab\n",
        "performs convolutions on multiple scales and uses the features from various scales to obtain a score map. The score map is interpolated and passed through a **conditional random field (CRF)** for final segmentation.\n",
        "\n",
        "The image scale processing is performed by processing images of various sizes with its CNN or parallel convolutions with varying level of dilated convolutions.\n",
        "\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/deeplab.JPG?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "OYT_omimbYpV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##RefiNet\n",
        "When it comes to high-resolution pictures, the computational compelxity increases and causes problem for dilated convolutions because dilated convolutions need bigger input and they are memory intensive.\n",
        "\n",
        "RefiNet is proposed to overcome this problem:\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/refinet.JPG?raw=true)\n",
        "\n",
        "RefiNet applies the encoder-decoder structure: the encoder outputs a CNN and the decoder concatenates the features of various sizes.\n",
        "The concatenation is done by upscaling the low dimensional feature\n",
        "\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/refinet2.JPG?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "h77NQ-W3c7NG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##PSPnet\n",
        "increases the kernel size of pooling layers. The pooling is carried in a pyramid shape. The pyramid covers various portions and sizes of the images simultaneously. The loss function in-between the architecture enables moderate supervision.\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/pspnet.JPG?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "AMh0yPDCdfps",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Large Kernels\n",
        "have bigger receptive fields than small kernels. The\n",
        "computational complexity of these large kernels can be used to overcome with\n",
        "an approximate smaller kernel.\n",
        "\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/large_kernel.JPG?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "KKjMfZ-2d5ug",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##DeepLab V3\n",
        "In this updated version, the concept of batch normalization is applied to improve the performance.\n",
        "\n",
        "The multi-scale of the feature is encoded in a cascaded fashion to improve the performance:\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/deeplab_v3.JPG?raw=true)\n",
        "\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/deeplab_v3_2.JPG?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "IGcb3g4Senr6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Ultra-Nerve Segmentation\n",
        "The original dataset of the segmentation of the nerve structure from ultrasound images of the neck can be download from https://www.kaggle.com/c/ultrasound-nerve-segmentation.\n",
        "\n",
        "The UNET model resembles an autoencoder but with convolutions instead of a fully connected layer. The encoding part with convolution decreases the dimensions and the decoder part increases dimensions:\n",
        "\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/unet.JPG?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "6Kkka0YF_ipq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The convolutions of the similar sized encoder and decoder part are learned with skipping connections.\n",
        "\n",
        "The model output is a mask ranging from 0 to 1.\n",
        "\n",
        "Let's start with data preparation"
      ]
    },
    {
      "metadata": {
        "id": "tqr3nPkBC9Jt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from skimage.io import imsave, imread\n",
        "\n",
        "data_path = './data/ch5'\n",
        "\n",
        "image_rows = 420\n",
        "image_cols = 580\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qc9KJUwbAjia",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_train_data():\n",
        "  train_data_path = os.path.join(data_path, 'train')\n",
        "  images = os.listdir(train_data_path)\n",
        "  total = int(len(images) / 2)\n",
        "  \n",
        "  imgs = np.ndarray((total, image_rows, image_cols), dtype=np.uint8)\n",
        "  imgs_mask = np.ndarray((total, image_rows, image_cols), dtype=np.uint8)\n",
        "  \n",
        "  i = 0\n",
        "  print('-'*30)\n",
        "  print('Creating training images...')\n",
        "  print('-'*30)\n",
        "  for image_name in images:\n",
        "    if 'mask' in image_name:\n",
        "      continue\n",
        "    image_mask_name = image_name.split('.')[0] + '_mask.tif'\n",
        "    img = imread(os.path.join(train_data_path, image_name), as_grey=True)\n",
        "    img_mask = imread(os.path.join(train_data_path, image_mask_name), as_grey=True)\n",
        "    \n",
        "    img = np.array([img])\n",
        "    img_mask = np.array([img_mask])\n",
        "    \n",
        "    imgs[i] = img\n",
        "    imgs_mask[i] = img_mask\n",
        "    \n",
        "    if i%100 == 0:\n",
        "      print('Done: {0}/{1} images'.format(i, total))\n",
        "    i += 1\n",
        "  print('Loading done.')\n",
        "  \n",
        "  np.save('imgs_train.npy', imgs)\n",
        "  np.save('imgs_mask_train.npy', imgs_mask)\n",
        "  print('Saving to .npy files done.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gi30f2r8DAIv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_train_data():\n",
        "  imgs_train = np.load('imgs_train.npy')\n",
        "  imgs_mask_train = np.load('imgs_mask_train.npy')\n",
        "  return imgs_train, imgs_mask_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a1luGU7aDMIf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_test_data():\n",
        "  test_data_path = os.path.join(data_path, 'test')\n",
        "  images = os.listdir(test_data_path)\n",
        "  total = len(images)\n",
        "  \n",
        "  imgs = np.ndarray((total, image_rows, image_cols), dtype=np.uint8)\n",
        "  imgs_id = np.ndarray((total, ), dtype=np.int32)\n",
        "  \n",
        "  i = 0\n",
        "  print('-'*30)\n",
        "  print('Creating test images...')\n",
        "  print('-'*30)\n",
        "  for image_name in images:\n",
        "    img_id = int(image_name.split('.')[0])\n",
        "    img = imread(os.path.join(train_data_path, image_name), as_grey=True)\n",
        "    \n",
        "    img = np.array([img])\n",
        "    \n",
        "    imgs[i] = img\n",
        "    imgs_id[i] = img_id\n",
        "    \n",
        "    if i%100 == 0:\n",
        "      print('Done: {0}/{1} images'.format(i, total))\n",
        "    i += 1\n",
        "  print('Loading done.')\n",
        "  \n",
        "  np.save('imgs_test.npy', imgs)\n",
        "  np.save('imgs_id_test.npy', imgs_id)\n",
        "  print('Saving to .npy files done.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RSWzOUjDERDr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_test_data():\n",
        "  imgs_test = np.load('imgs_test.npy')\n",
        "  imgs_id = np.load('imgs_id_test.npy')\n",
        "  return imgs_test, imgs_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gAO336KXFFT7",
        "colab_type": "code",
        "outputId": "d56aa3ba-af5f-4e29-d264-4d561fcb128c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "cell_type": "code",
      "source": [
        "create_train_data()\n",
        "create_test_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Creating training images...\n",
            "------------------------------\n",
            "Done: 0/5635 images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/external/tifffile/tifffile.py:2611: RuntimeWarning: py_decodelzw encountered unexpected end of stream\n",
            "  strip = decompress(strip)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done: 100/5635 images\n",
            "Done: 200/5635 images\n",
            "Done: 300/5635 images\n",
            "Done: 400/5635 images\n",
            "Done: 500/5635 images\n",
            "Done: 600/5635 images\n",
            "Done: 700/5635 images\n",
            "Done: 800/5635 images\n",
            "Done: 900/5635 images\n",
            "Done: 1000/5635 images\n",
            "Done: 1100/5635 images\n",
            "Done: 1200/5635 images\n",
            "Done: 1300/5635 images\n",
            "Done: 1400/5635 images\n",
            "Done: 1500/5635 images\n",
            "Done: 1600/5635 images\n",
            "Done: 1700/5635 images\n",
            "Done: 1800/5635 images\n",
            "Done: 1900/5635 images\n",
            "Done: 2000/5635 images\n",
            "Done: 2100/5635 images\n",
            "Done: 2200/5635 images\n",
            "Done: 2300/5635 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EgTyhF5zFMHd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can define the model.\n",
        "\n",
        "Start with the size of images,"
      ]
    },
    {
      "metadata": {
        "id": "HnKhVPfs_uP0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imsave, imread\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IdTVJ_4lFXc-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_height, image_width = 96, 96\n",
        "smoothness = 1.0\n",
        "work_dir = './data/ch5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rmdNpxhYFkOa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the dice coefficients and its loss function"
      ]
    },
    {
      "metadata": {
        "id": "QzkgafYXFpk1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dice_coefficient(y1, y2):\n",
        "  y1 = tf.flatten(y1)\n",
        "  y2 = tf.flatten(y2)\n",
        "  return (2.*tf.sum(y1*y2)+smoothness) / (tf.sum(y1)+tf.sum(y2)+smoothness)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X92hI5k9F4W-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dice_coefficient_loss(y1, y2):\n",
        "  return -dice.coefficient(y1, y2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FkQRtI9_F-kl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the layers to be used:"
      ]
    },
    {
      "metadata": {
        "id": "NsdVRlgdF-T9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(imgs):\n",
        "  imgs_p = np.nadrray((imgs.shape[0], image_height, image_width), dtype=np.uint8)\n",
        "  for i in range(imgs.shape[0]):\n",
        "    imgs_p[i] = resize(imgs[i], (image_width, image_height), preserve_range=True)\n",
        "  imgs_p = imgs_p[..., np.newaxis]\n",
        "  return imgs_p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wDmmyr3sTerg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convolution_layer(filters, kernel=(3,3), activation='relu', input_shape=None):\n",
        "  if input_shape is None:\n",
        "    return tf.keras.layers.Conv2D(filters=filters,\n",
        "                                  kernel_size=kernel,\n",
        "                                  activation=activation)\n",
        "  else:\n",
        "    return tf.keras.layers.Conv2D(filters=filters,\n",
        "                                  kernel_size=kernel,\n",
        "                                  activation=activation,\n",
        "                                  input_shape=input_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qF-XrkINT2kl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def concatenated_deconvolution_layer(filters):\n",
        "  return tf.keras.layers.concatenate([\n",
        "      tf.keras.layers.Conv2DTranspose(filters=filters,\n",
        "                                      kernel=(2,2),\n",
        "                                      strides=(2,2),\n",
        "                                      padding='same')],\n",
        "      axis=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PPv4u4QyUgBK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pooling_layer():\n",
        "  return tf.keras.layers.MaxPooling2D(pool_size=(2,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JPLz0lnRUyCs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the UNET model:"
      ]
    },
    {
      "metadata": {
        "id": "7jWEK6fPUzqf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "unet = tf.keras.models.Sequential()\n",
        "inputs = tf.keras.layers.Input((image_height, image_width, 1))\n",
        "input_shape = (image_height, image_width, 1)\n",
        "unet.add(convolution_layer(32, input_shape=input_shape))\n",
        "unet.add(convolution_layer(32))\n",
        "unet.add(pooling_layer())\n",
        "\n",
        "unet.add(convolution_layer(64))\n",
        "unet.add(convolution_layer(64))\n",
        "unet.add(pooling_layer())\n",
        "\n",
        "unet.add(convolution_layer(128))\n",
        "unet.add(convolution_layer(128))\n",
        "unet.add(pooling_layer())\n",
        "\n",
        "unet.add(convolution_layer(256))\n",
        "unet.add(convolution_layer(256))\n",
        "unet.add(pooling_layer())\n",
        "\n",
        "unet.add(convolution_layer(512))\n",
        "unet.add(convolution_layer(512))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XAwbt8wdVkqC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "unet.add(concatenated_deconvolution_layer(256))\n",
        "unet.add(convolution_layer(256))\n",
        "unet.add(convolution_layer(256))\n",
        "\n",
        "unet.add(concatenated_deconvolution_layer(128))\n",
        "unet.add(convolution_layer(128))\n",
        "unet.add(convolution_layer(128))\n",
        "\n",
        "unet.add(concatenated_deconvolution_layer(64))\n",
        "unet.add(convolution_layer(64))\n",
        "unet.add(convolution_layer(64))\n",
        "\n",
        "unet.add(concatenated_deconvolution_layer(32))\n",
        "unet.add(convolution_layer(32))\n",
        "unet.add(convolution_layer(32))\n",
        "\n",
        "unet.add(convolution_layer(1, kernel=(1,1), activation='sigmoid'))\n",
        "\n",
        "unet.summary()\n",
        "unet.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
        "             loss=dice_coefficient_loss,\n",
        "             metrics=[dice_coefficient])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nww0ECOoWBP0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train the model using the training images:"
      ]
    },
    {
      "metadata": {
        "id": "wrgJE42pWFNA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train, y_train_mask = load_train_data()\n",
        "\n",
        "# Preprocessing the training images first\n",
        "x_train = preprocess(x_train)\n",
        "y_train_mask = preprocess(y_train_mask)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "mean = np.mean(x_train)\n",
        "std = np.std(x_train)\n",
        "\n",
        "x_train -= mean\n",
        "x_train /= std\n",
        "\n",
        "y_train_mask = y_train_mask.astype('float32')\n",
        "y_train_mask /= 255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yr3tklsBWuh1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "unet.fit(x_train, \n",
        "         y_train_mask, \n",
        "         batch_size=32, \n",
        "         epochs=20, \n",
        "         verbose=1, \n",
        "         shuffle=True,\n",
        "         validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lJHnALRDW8m-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After training, start to test the model performance using the testing data,"
      ]
    },
    {
      "metadata": {
        "id": "5zoksnceXDJ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_test, y_test_mask = load_test_data()\n",
        "\n",
        "x_test = preprocess(x_test)\n",
        "x_test = x_test.astype('float32')\n",
        "x_test -= mean\n",
        "x_test /= std\n",
        "\n",
        "y_test_pred = unet.predict(x_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UaUc8oHlXaS1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save the prediction result\n",
        "for image, image_id in zip(y_test_pred, y_test_mask):\n",
        "  image = (image[:,:,0] * 255.).astype(np.uint8)\n",
        "  imsave(os.path.join(work_dir, str(image_id)+'.png'), image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QkSORGijZH99",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#FCN Model for Segmentation"
      ]
    },
    {
      "metadata": {
        "id": "VYSJOKnPaSQo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from .resnet50 import ResNet50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HESbG2IiaeMK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_labels = 6\n",
        "input_shape = [28, 28]\n",
        "\n",
        "img_height, img_width, _ = input_shape\n",
        "input_tensor = tf.keras.layers.Input(shape=input_shape)\n",
        "weights = 'imagenet'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ssgL_lPAaryq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Inititalize the `ResNet` model:"
      ]
    },
    {
      "metadata": {
        "id": "OhE9oFhZawkH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "resnet50_model = ResNet50(include_top=False,\n",
        "                          weights=weights,\n",
        "                          input_tensor=input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ao4m44zib2k6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Take the last three layers from `ResNet` model:"
      ]
    },
    {
      "metadata": {
        "id": "vOKQ3qYzb7Ds",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "final_32 = resnet50_model.get_layer('final_32').output\n",
        "final_16 = resnet50_model.get_layer('final_16').output\n",
        "final_x8 = resnet50_model.get_layer('final_x8').output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VKObjeebcMI0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each skip connection has to be compressed to match the channel that is equal to\n",
        "the number of labels:"
      ]
    },
    {
      "metadata": {
        "id": "N1h_NM03cQHX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "c32 = tf.keras.layers.Conv2D(nb_labels, (1,1))(final_32)\n",
        "c16 = tf.keras.layers.Conv2D(nb_labels, (1,1))(final_16)\n",
        "c8 = tf.keras.layers.Conv2D(nb_labels, (1,1))(final_x8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0guyRJF7cUyy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The output of the compressed skip connection can be resized using **bilinear\n",
        "interpolation**. The interpolation can be implemented by using a `Lambda` layer that\n",
        "can compute TensorFlow operation."
      ]
    },
    {
      "metadata": {
        "id": "A6UizjF1cdRU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def resize_bilinear(images):\n",
        "  return tf.image.resize_bilinear(images, [img_height, img_width])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IFqUZSaoclKZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "r32 = tf.keras.layers.Lambda(resize_bilinear)(c32)\n",
        "r16 = tf.keras.layers.Lambda(resize_bilinear)(c16)\n",
        "r8 = tf.keras.layers.Lambda(resize_bilinear)(c8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pijm0JJecoz0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Merge these three layers by adding those three values,"
      ]
    },
    {
      "metadata": {
        "id": "aqWb6KAacwQM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = tf.keras.layers.Add()([r32, r16, r8])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LEY0oHeBcyx4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The probabilities of the model can be applied using softmax activation. The\n",
        "model is resized before and after applying softmax:"
      ]
    },
    {
      "metadata": {
        "id": "eZ0mOjCgc0xC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.keras.ayers.Reshape((img_height * img_width, nb_labels))(m)\n",
        "x = tf.keras.layers.Activation('img_height')(x)\n",
        "x = tf.keras.layers.Reshape((img_height, img_width, nb_labels))(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "la1S6nVdc21X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fcn_model = tf.keras.models.Model(input=input_tensor, output=x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uw11fI9Vc35z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Thus, a simple FCN model has been defined."
      ]
    },
    {
      "metadata": {
        "id": "yxPw63ZxdCkG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Instance Segmentation\n",
        "This process of separating the required information from the rest is\n",
        "widely known as **segmenting instances**. During this process, the input image is\n",
        "first taken, then the bounding box will be localized with the objects and at last, a\n",
        "pixel-wise mask will be predicted for each of the class. For each of the objects,\n",
        "pixel-level accuracy is calculated.\n",
        "\n",
        "**Mask RCNN**: \n",
        "\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/mask_rcnn.JPG?raw=true)\n",
        "\n",
        "The architecture looks similar to the R-CNN with an addition of segmentation. It\n",
        "is a multi-stage network with end-to-end training. The region proposals are\n",
        "learned. The network is split into two, one for detection and the other for a\n",
        "classification score."
      ]
    }
  ]
}