{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch2_image_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/deep_learning_for_computer_vision/blob/master/ch2_image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "1vlV5lSh92Ov",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0imm9wUVIlJk",
        "colab_type": "code",
        "outputId": "b3433df3-dd8e-450a-fc7f-455a5bff9715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My' 'Drive/Colab' 'Notebooks/Deep_Learning_for_Computer_Vision/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Colab Notebooks/Deep_Learning_for_Computer_Vision\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pyVLr3B-86uC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Image classification** is the task of classifying a whole image as a single label."
      ]
    },
    {
      "metadata": {
        "id": "dKS-QJFt9GvK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#The MNIST datasets\n",
        "has handwritten digits from 0-9 with 60,000 images for training and 10,000 images for testing. The images are normalized to the size of 28 image pixels by 28\n",
        "image pixels, converted to grey size, and centered to a fixed size. This is a small\n",
        "dataset on which an algorithm can be quickly tested."
      ]
    },
    {
      "metadata": {
        "id": "M1fUAlQJ9Vw6",
        "colab_type": "code",
        "outputId": "20c42302-7d83-4b7b-ebd0-7781f838ef0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ifU9dDLb9o1T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Perceptron\n",
        "A single-layer neural network."
      ]
    },
    {
      "metadata": {
        "id": "GIAXxvNM966X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "no_classes = 10\n",
        "batch_size = 100\n",
        "total_batches = 200\n",
        "\n",
        "x_input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
        "y_input = tf.placeholder(tf.float32, shape=[None, no_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IiBBY2IH-M0i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The **None** in the `shape` argument indicates it can be of any size as we have not yet defined the batch size.\n",
        "\n",
        "The second argument is the size of the tensor for `x_input` and the number of classes for `y_input`."
      ]
    },
    {
      "metadata": {
        "id": "3RHTI0N7-e7I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Define the variables for a fully connected layer"
      ]
    },
    {
      "metadata": {
        "id": "04wDu-mC-oTR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights = tf.Variable(tf.random_normal([input_size, no_classes]))\n",
        "bias = tf.Variable(tf.random_normal([no_classes]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UoPUUaZF-ykq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The initialization of the variables can be zeroes but a random normal distribution\n",
        "gives a steady training."
      ]
    },
    {
      "metadata": {
        "id": "ApNMERjY-1Rd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logits = tf.matmul(x_input, weights) + bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pkDGePQH-50w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The inputs are weighted and added with the bias to produce `logits`.\n",
        "\n",
        "The `logits` produced by the perceptron has to be compared against one-hot labels `y_input`. For multi-label classification, it is better to use softmax coupled with cross-entropy for comparing `logits` and one-hot labels. That is why we use `tf.nn.softmax_cross_entropy_with_logits` here."
      ]
    },
    {
      "metadata": {
        "id": "i3Wvg09h_bu9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_input, logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ITF8oIoq_ki1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The loss can be computed by averaging the cross-entropies. Then the cross-entropy is fed through gradient descent optimization done by `tf.train.GradientDescentOptimizer`. The optimizer takes the loss and minimizes it with a learning rate of `0.5`."
      ]
    },
    {
      "metadata": {
        "id": "lxgqNmpu_5EJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_operation = tf.reduce_mean(softmax_cross_entropy)\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss_operation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z8yF1kSsAGZv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The softmax and cross-entropies are computed together from the `tf.nn` package,\n",
        "which has several other useful methods. The `tf.train` has several optimizers, and here, we are using the vanilla gradient descent."
      ]
    },
    {
      "metadata": {
        "id": "2UiepckNATVg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Adam optimizer is particularly useful for computer vision applications. It generally converges faster and we need not define a learning rate to start with."
      ]
    },
    {
      "metadata": {
        "id": "iY-wc09xAclJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Train the model with data\n",
        "After defining the model and training operation, the next step is to start training the model with datasets. During training, the gradients are calculated and the weights are updated. The variables have not yet been initialized. \n",
        "\n",
        "Start the session and initialize the variables using a global variable initializer:"
      ]
    },
    {
      "metadata": {
        "id": "8z2LcYr6A2fP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "session = tf.Session()\n",
        "session.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oKnTk-C0BDr0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The preceding two lines are mostly required to run in order to generate the computational graph in TensorFlow.\n",
        "\n",
        "Now the\n",
        "graph is ready to be fed with data and start training. Over a loop, read the data in\n",
        "batches and train the model. The optimizer has to be called in order for the\n",
        "graph to update the weights."
      ]
    },
    {
      "metadata": {
        "id": "KLhaWo17BU-Q",
        "colab_type": "code",
        "outputId": "9bc8a96b-47e2-4ad8-ff51-c2639b89717b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3417
        }
      },
      "cell_type": "code",
      "source": [
        "for batch_no in range(total_batches):\n",
        "  mnist_batch = mnist_data.train.next_batch(batch_size)\n",
        "  train_images, train_labels = mnist_batch[0], mnist_batch[1]\n",
        "  _, loss_value = session.run([optimizer, loss_operation], \n",
        "                              feed_dict={x_input: train_images,\n",
        "                                         y_input: train_labels})\n",
        "  print(loss_value)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15.425655\n",
            "12.528323\n",
            "10.212998\n",
            "9.878272\n",
            "8.96583\n",
            "9.350922\n",
            "8.801382\n",
            "7.9197383\n",
            "6.283884\n",
            "7.368129\n",
            "7.187389\n",
            "5.4572115\n",
            "5.6831303\n",
            "4.525123\n",
            "4.8468604\n",
            "4.07745\n",
            "5.152235\n",
            "4.097923\n",
            "4.1719036\n",
            "4.221296\n",
            "3.9882026\n",
            "3.583871\n",
            "2.7037687\n",
            "2.7459028\n",
            "3.4026573\n",
            "3.5995405\n",
            "3.7862546\n",
            "2.7969053\n",
            "3.0131173\n",
            "2.8292258\n",
            "3.6021082\n",
            "2.5368512\n",
            "2.8336418\n",
            "2.6284442\n",
            "2.4189205\n",
            "3.0058658\n",
            "2.3350453\n",
            "1.8733461\n",
            "1.8531623\n",
            "2.382483\n",
            "2.377443\n",
            "2.131122\n",
            "1.7753848\n",
            "3.292028\n",
            "2.3170593\n",
            "2.3126333\n",
            "2.231833\n",
            "2.161497\n",
            "2.5463414\n",
            "1.322619\n",
            "2.1830735\n",
            "2.2246692\n",
            "2.0370018\n",
            "1.9545343\n",
            "2.0228379\n",
            "1.1891007\n",
            "1.7369035\n",
            "2.158346\n",
            "1.9418837\n",
            "2.1336083\n",
            "1.633956\n",
            "1.110815\n",
            "1.5804164\n",
            "1.4515866\n",
            "1.4303087\n",
            "1.9055177\n",
            "1.4277269\n",
            "1.5750102\n",
            "1.38172\n",
            "1.5686188\n",
            "1.2858803\n",
            "1.2129692\n",
            "1.6078556\n",
            "1.6003119\n",
            "1.512208\n",
            "2.433582\n",
            "1.8361506\n",
            "1.1280355\n",
            "1.6376735\n",
            "1.696698\n",
            "1.4147207\n",
            "1.1470606\n",
            "1.907039\n",
            "1.9964045\n",
            "2.0070803\n",
            "1.236176\n",
            "1.1887876\n",
            "0.9960818\n",
            "1.0750276\n",
            "1.4116774\n",
            "0.9897826\n",
            "1.1516329\n",
            "1.654535\n",
            "1.4748168\n",
            "1.3473212\n",
            "1.6851028\n",
            "1.2305746\n",
            "1.8957536\n",
            "1.5888234\n",
            "1.6476947\n",
            "1.3139136\n",
            "1.6029781\n",
            "1.628187\n",
            "1.5730563\n",
            "1.0455792\n",
            "1.8008809\n",
            "1.2349081\n",
            "1.1791841\n",
            "2.1485615\n",
            "1.2717257\n",
            "1.3940624\n",
            "1.5842264\n",
            "1.5520272\n",
            "1.4733925\n",
            "1.7935803\n",
            "0.98577124\n",
            "1.2910603\n",
            "1.733521\n",
            "0.9371229\n",
            "1.2081145\n",
            "1.6384952\n",
            "0.93291616\n",
            "1.378456\n",
            "1.1986183\n",
            "1.3641396\n",
            "1.839893\n",
            "1.3502283\n",
            "0.90666527\n",
            "1.1706166\n",
            "0.9493815\n",
            "1.319174\n",
            "1.3561243\n",
            "1.1279535\n",
            "0.9336972\n",
            "1.0522763\n",
            "1.0999612\n",
            "1.0068028\n",
            "1.6302934\n",
            "1.2315931\n",
            "1.278997\n",
            "1.2761073\n",
            "0.88055617\n",
            "1.1490722\n",
            "1.1466241\n",
            "1.4294406\n",
            "1.23284\n",
            "1.2697488\n",
            "1.3843079\n",
            "0.7263892\n",
            "1.2945992\n",
            "0.7795323\n",
            "1.2719736\n",
            "1.175544\n",
            "0.8851898\n",
            "1.015123\n",
            "1.8595489\n",
            "1.345827\n",
            "1.2715513\n",
            "1.1167064\n",
            "1.0693756\n",
            "1.491774\n",
            "0.7829589\n",
            "1.0149573\n",
            "1.202823\n",
            "0.7523926\n",
            "1.0599157\n",
            "1.002256\n",
            "0.81428397\n",
            "1.115046\n",
            "1.5759093\n",
            "0.9690393\n",
            "1.1908177\n",
            "1.2465439\n",
            "0.85889244\n",
            "0.6518267\n",
            "1.1028153\n",
            "1.5358063\n",
            "1.1134355\n",
            "0.83748156\n",
            "1.1576102\n",
            "1.1884046\n",
            "0.63701224\n",
            "1.3709487\n",
            "0.85271895\n",
            "1.283827\n",
            "1.1219964\n",
            "0.81567\n",
            "1.1274756\n",
            "0.7480892\n",
            "0.98434496\n",
            "0.92456865\n",
            "1.1072048\n",
            "0.9146491\n",
            "1.1651454\n",
            "1.1463094\n",
            "1.2640346\n",
            "0.92744124\n",
            "0.8260775\n",
            "0.8874523\n",
            "1.2139741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qltHG7mqB_S4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first argument of the `run` method of the `session` function can have an array for which the outputs of\n",
        "the values are requested. We pass loss because printing loss tells us whether the\n",
        "model is getting trained or not. The loss is expected to decrease as we are\n",
        "minimizing the loss."
      ]
    },
    {
      "metadata": {
        "id": "_A7DiDGOCM1A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After training, we can evaluate how well the model worked by computing the accuracy."
      ]
    },
    {
      "metadata": {
        "id": "sllYYcp1CScG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = tf.argmax(logits, 1)\n",
        "correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1))\n",
        "accuracy_operation = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PwjXOwinCuyY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The prediction should be the index of the maximum activation. It should be\n",
        "compared with the ground truth on MNIST labels for correct predictions. The\n",
        "accuracy is calculated using the average of correct predictions. The accuracy of\n",
        "the data can be evaluated by running the session with test data as the feed\n",
        "dictionary."
      ]
    },
    {
      "metadata": {
        "id": "3_JfCFR0C0Pw",
        "colab_type": "code",
        "outputId": "fba1e5ce-e4f2-4ae0-d9ec-f427beb76cea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels\n",
        "accuracy_value = session.run(accuracy_operation, \n",
        "                             feed_dict={x_input: test_images,\n",
        "                                        y_input: test_labels})\n",
        "print('Accuracy: ', accuracy_value)\n",
        "session.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.8097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2ij2bMIFDSZo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Multiilayer Convolutional Network\n",
        "Create a multilayer convolutional network in\n",
        "TensorFlow and watch how a deeper network improves classification accuracy.\n",
        "\n",
        "In this section, we will use TensorBoard for visualizing the training process.\n",
        "In order to visualize the statistics of the variables, the values of variable statistics\n",
        "have to be added to `tf.summary`."
      ]
    },
    {
      "metadata": {
        "id": "wRjKjHOlEtzu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q4dOvejh9Dy5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
        "y_input = tf.placeholder(tf.float32, shape=[None, no_classes])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A_Gpo0OeD-XB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_variable_summary(tf_variable, summary_name):\n",
        "  with tf.name_scope(summary_name + '_summary'):\n",
        "    mean = tf.reduce_mean(tf_variable)\n",
        "    tf.summary.scalar('Mean', mean)\n",
        "    with tf.name_scope('standard_deviation'):\n",
        "      standard_deviation = tf.sqrt(tf.reduce_mean(tf.square(tf_variable - mean)))\n",
        "    tf.summary.scalar('StandardDeviation', standard_deviation)\n",
        "    tf.summary.scalar('Maximum', tf.reduce_max(tf_variable))\n",
        "    tf.summary.scalar('Minimum', tf.reduce_min(tf_variable))\n",
        "    tf.summary.histogram('Histogram', tf_variable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ereOyGYAEzAw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The variable `summary` function writes the summaries of a variable. There are five\n",
        "statistics added to the summaries: mean, standard deviation, maximum,\n",
        "minimum and histogram. Summaries can be either a `scalar` or a `histogram`."
      ]
    },
    {
      "metadata": {
        "id": "I0N8xl50FANm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unlike fully-connected network, we wil resize the `MNIST` data intor a square matrix and use it like a 2D image."
      ]
    },
    {
      "metadata": {
        "id": "Hp7dzieuEsp_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_input_reshape = tf.reshape(x_input, [-1, 28, 28, 1], name='input_reshape')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w2Gh0sCqFUDm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The dimension -1 denotes that the batch size can be any number. The `name` argument will be reflected in the TensorBoard graph for ease of understanding.\n",
        "\n",
        "Define the convolution layer,"
      ]
    },
    {
      "metadata": {
        "id": "mHWKm06z6IX8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convolution_layer(input_layer, filters, kernel_size=[3,3], activation=tf.nn.relu):\n",
        "  layer = tf.layers.conv2d(inputs=input_layer,\n",
        "                           filters=filters,\n",
        "                           kernel_size=kernel_size,\n",
        "                           activation=activation)\n",
        "  \n",
        "  add_variable_summary(layer, 'convolution')\n",
        "  return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L_JFW1B46e2r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The summaries are\n",
        "added to the layer within the function and the layer is returned.\n",
        "\n",
        "Next, define pooling layer,"
      ]
    },
    {
      "metadata": {
        "id": "DAIYVZ546nJs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pooling_layer(input_layer, pool_size=[2,2], strides=2):\n",
        "  layer = tf.layers.max_pooling2d(inputs=input_layer,\n",
        "                                 pool_size=pool_size,\n",
        "                                 strides=strides)\n",
        "  \n",
        "  add_variable_summary(layer, 'pooling')\n",
        "  return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JnMrPkQA68TH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Move on and define the fully connected layer,"
      ]
    },
    {
      "metadata": {
        "id": "UQBC2hqw7APT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dense_layer(input_layer, units, activation=tf.nn.relu):\n",
        "  layer = tf.layers.dense(inputs=input_layer,\n",
        "                          units=units,\n",
        "                          activation=activation)\n",
        "  add_variable_summary(layer, 'dense')\n",
        "  return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mQoxyuv97QAd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `pooling_layer` takes the feature map from the convolution layer and reduces it to half its size by skipping, using the pool size\n",
        "and strides. All these layers are connected as a graph and are just defined. None\n",
        "of the values is initialized. Another convolution layer can be added to transform\n",
        "the sampled features from the first convolution layer to better features. After\n",
        "pooling, we may reshape the activations to a linear fashion in order to be fed\n",
        "through dense layers:"
      ]
    },
    {
      "metadata": {
        "id": "0kSiqQss7Z5P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "convolution_layer_1 = convolution_layer(x_input_reshape, 64)\n",
        "pooling_layer_1 = pooling_layer(convolution_layer_1)\n",
        "convolution_layer_2 = convolution_layer(pooling_layer_1, 128)\n",
        "pooling_layer_2 = pooling_layer(convolution_layer_2)\n",
        "flattened_pool = tf.reshape(pooling_layer_2, [-1, 5*5*128], name='flattened_pool')\n",
        "dense_layer_bottleneck = dense_layer(flattened_pool, 1024)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DiUzAm_59hEP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The only difference between the convolution layers is the filter size. It's\n",
        "important that the dimensions change appropriately from layer to layer.\n",
        "Choosing the parameters for kernel and stride are arbitrary and these numbers\n",
        "are chosen by experience. Two convolution layers are defined, and this can be\n",
        "followed by a fully connected layer. A dense-layer API can take any vector of a\n",
        "single dimension and map it to any number of hidden units, as in this case is 1024.\n",
        "\n",
        "Variable summaries are added for this layer as well.\n",
        "\n",
        "The dense layer is followed by a dropout layer with a rate of dropping out. Keeping this high will stop the network from learning. The training mode can be set to `True` and `False` based on when we use this. We will set this as `True` (default is `False`) for the\n",
        "training. We will have to change this while the accuracy is calculated."
      ]
    },
    {
      "metadata": {
        "id": "cKJo4I_B-FCE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dropout_bool = tf.placeholder(tf.bool)\n",
        "dropout_layer = tf.layers.dropout(inputs=dense_layer_bottleneck,\n",
        "                                  rate=0.4,\n",
        "                                  training=dropout_bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n58EzMvm-R_s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The dropout layer is fed again to a dense layer, which is called logits. Logits is\n",
        "the final layer with activations leading to the number of classes. The activations\n",
        "will be spiked for a particular class, which is the target class, and can be obtained\n",
        "for a maximum of those 10 activations:"
      ]
    },
    {
      "metadata": {
        "id": "f3zjSVT5-YCI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logits = dense_layer(dropout_layer, no_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pgFfOPIX-d2m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pass the logits through the softmax layer and calculate the cross-entropy.\n",
        "\n",
        "Use a scope name to get a better visualization in TensorBoard,"
      ]
    },
    {
      "metadata": {
        "id": "kII8Y8Sa-pxh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('loss'):\n",
        "  softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_input,\n",
        "                                                                  logits=logits)\n",
        "  loss_operation = tf.reduce_mean(softmax_cross_entropy, name='loss')\n",
        "  tf.summary.scalar('loss', loss_operation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vryjT_RkBoG8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This `loss` function can be optimized with `tf.train` APIs' methods."
      ]
    },
    {
      "metadata": {
        "id": "WtpQjd35B8GW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('optimizer'):\n",
        "  optimizer = tf.train.AdamOptimizer().minimize(loss_operation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FZBQF0nkCGZD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the accuracy,"
      ]
    },
    {
      "metadata": {
        "id": "aKQGfiIJCQAu",
        "colab_type": "code",
        "outputId": "3057736f-499d-4e0f-fd78-4263f4bf39c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('accuracy'):\n",
        "  with tf.name_scope('correct_prediction'):\n",
        "    predictions = tf.argmax(logits, 1)\n",
        "    correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1))\n",
        "  with tf.name_scope('accuracy'):\n",
        "    accuracy_operation = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "    \n",
        "tf.summary.scalar('accuracy', accuracy_operation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'accuracy_1:0' shape=() dtype=string>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "metadata": {
        "id": "WcmmpcfaC0Sy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A scalar summary for accuracy is also added.\n",
        "\n",
        "Next, start a session and initialize the variables."
      ]
    },
    {
      "metadata": {
        "id": "KSurB-jMCz7z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "session = tf.Session()\n",
        "session.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QyDL4fa_C-1E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The summaries have to be merged, and the files for writing the training and\n",
        "testing summaries have to be defined:"
      ]
    },
    {
      "metadata": {
        "id": "J4Q6Z7nVCUbm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "merged_summary_operation = tf.summary.merge_all()\n",
        "train_summary_writer = tf.summary.FileWriter('./tmp/train', session.graph)\n",
        "test_summary_writer = tf.summary.FileWriter('./tmp/test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hHRsyaxtDRJP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that the graph is written once with the `summary_writer`.\n",
        "\n",
        "Next, load the data in batches and start the training phase,"
      ]
    },
    {
      "metadata": {
        "id": "aNkKbWq5DaU1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels\n",
        "\n",
        "for batch_no in range(total_batches):\n",
        "  mnist_batch = mnist_data.train.next_batch(batch_size)\n",
        "  train_images, train_labels = mnist_batch[0], mnist_batch[1]\n",
        "  _, merged_summary = session.run([optimizer, merged_summary_operation],\n",
        "                                  feed_dict={x_input:train_images,\n",
        "                                             y_input:train_labels,\n",
        "                                             dropout_bool:True})\n",
        "  train_summary_writer.add_summary(merged_summary, batch_no)\n",
        "  \n",
        "  if batch_no % 10 == 0:\n",
        "    merged_summary, _ = session.run([merged_summary_operation, accuracy_operation],\n",
        "                                    feed_dict={x_input:test_images,\n",
        "                                               y_input:test_labels,\n",
        "                                               dropout_bool:False})\n",
        "    test_summary_writer.add_summary(merged_summary, batch_no)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F3Ab3HMGEwRA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Summaries are returned in every iteration for training data and are added to the\n",
        "writer. For every tenth iteration, the test summaries are added. Note that the\n",
        "dropout is enabled only during training and not during testing."
      ]
    },
    {
      "metadata": {
        "id": "pbCJ5uwiE0cL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Use TensorBoard to Visualize the Network\n",
        "Please consult the method shown in my website to set up the Tensorboard in Colab\n",
        "https://sites.google.com/ncsu.edu/binliu14/computational-intelligence?authuser=0"
      ]
    },
    {
      "metadata": {
        "id": "mo40mKXOE99S",
        "colab_type": "code",
        "outputId": "7cffe6c1-81e0-4124-9b8b-a5a000211550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "# install ngrok\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-28 21:09:48--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.86.186.182, 52.2.175.150, 52.201.75.180, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.86.186.182|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]   5.11M  15.7MB/s    in 0.3s    \n",
            "\n",
            "2018-11-28 21:09:48 (15.7 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [5363700/5363700]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k1LtsnS1Gbfz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run TensorBoard,\n",
        "# locating the summary file\n",
        "# training summary -> ./tmp/train\n",
        "# testing summary -> ./tmp/test\n",
        "LOG_DIR = './tmp'\n",
        "get_ipython().system_raw(\n",
        "      'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format(LOG_DIR))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VwsrfF6WG-1A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run ngrok\n",
        "# run ngrok to tunnel TensorBoard port 6006 to the outside world\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_GGCfHs6HI7L",
        "colab_type": "code",
        "outputId": "213ce35d-fcb4-4f3b-9f80-0d8c12e2a0b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Get URL\n",
        "# access the colab TensorBoard web page\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://f57e6b17.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OnZ1R29iKKXv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Train MNIST Model in Keras\n",
        "Build the same model using the `tf.keras` APIs"
      ]
    },
    {
      "metadata": {
        "id": "hjWNAeZIL6dd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reset graph\n",
        "tf.reset_default_graph()\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CqxerAKAKwzR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "no_classes = 10\n",
        "epochs = 50\n",
        "image_height, image_width = 28, 28\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], image_height, image_width, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], image_height, image_width, 1)\n",
        "input_shape = (image_height, image_width, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, no_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, no_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Twr1HnZxMMM5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def simple_cnn(input_shape):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Conv2D(filters=64,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   activation='relu',\n",
        "                                   input_shape=input_shape)\n",
        "           )\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(tf.keras.layers.Conv2D(filters=128,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   activation='relu')\n",
        "           )\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(units=1024, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(rate=0.4))\n",
        "  model.add(tf.keras.layers.Dense(units=no_classes, activation='softmax'))\n",
        "  \n",
        "  model.summary()\n",
        "  model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_uypc51EODjO",
        "colab_type": "code",
        "outputId": "3aa15cab-6c01-4ee9-c484-11257ba7422c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "simple_cnn_model = simple_cnn(input_shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3200)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              3277824   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 3,362,570\n",
            "Trainable params: 3,362,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lqfi3JrQOQ3r",
        "colab_type": "code",
        "outputId": "6ce49d6f-3c45-4bbc-a382-7755d133db1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "cell_type": "code",
      "source": [
        "simple_cnn_model.fit(x_train, y_train, batch_size, epochs, (x_test, y_test))\n",
        "train_loss, train_accuracy = simple_cnn_model.evaluate(x_train,\n",
        "                                                       y_train,\n",
        "                                                       verbose=0)\n",
        "print('Train data loss: ', train_loss)\n",
        "print('Train data accuracy: ', train_accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "Epoch 2/50\n",
            "Epoch 3/50\n",
            "Epoch 4/50\n",
            "Epoch 5/50\n",
            "Epoch 6/50\n",
            "Epoch 7/50\n",
            "Epoch 8/50\n",
            "Epoch 9/50\n",
            "Epoch 10/50\n",
            "Epoch 11/50\n",
            "Epoch 12/50\n",
            "Epoch 13/50\n",
            "Epoch 14/50\n",
            "Epoch 15/50\n",
            "Epoch 16/50\n",
            "Epoch 17/50\n",
            "Epoch 18/50\n",
            "Epoch 19/50\n",
            "Epoch 20/50\n",
            "Epoch 21/50\n",
            "Epoch 22/50\n",
            "Epoch 23/50\n",
            "Epoch 24/50\n",
            "Epoch 25/50\n",
            "Epoch 26/50\n",
            "Epoch 27/50\n",
            "Epoch 28/50\n",
            "Epoch 29/50\n",
            "Epoch 30/50\n",
            "Epoch 31/50\n",
            "Epoch 32/50\n",
            "Epoch 33/50\n",
            "Epoch 34/50\n",
            "Epoch 35/50\n",
            "Epoch 36/50\n",
            "Epoch 37/50\n",
            "Epoch 38/50\n",
            "Epoch 39/50\n",
            "Epoch 40/50\n",
            "Epoch 41/50\n",
            "Epoch 42/50\n",
            "Epoch 43/50\n",
            "Epoch 44/50\n",
            "Epoch 45/50\n",
            "Epoch 46/50\n",
            "Epoch 47/50\n",
            "Epoch 48/50\n",
            "Epoch 49/50\n",
            "Epoch 50/50\n",
            "Train data loss:  0.0009889797066601432\n",
            "Train data accuracy:  0.9998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wM64RyR-Oi7F",
        "colab_type": "code",
        "outputId": "bca5cf45-d012-4c8f-ee78-ba757f16a622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = simple_cnn_model.evaluate(x_test,\n",
        "                                                     y_test,\n",
        "                                                     verbose=0)\n",
        "print('Test data loss: ', test_loss)\n",
        "print('Test data accuracy: ', test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test data loss:  0.048683962830335555\n",
            "Test data accuracy:  0.9931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gC5AKXQIST-_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Deep Learning Models"
      ]
    },
    {
      "metadata": {
        "id": "BeaO7GthQ4lJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##AlexNet\n",
        "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
        "\n",
        "##VGG-16\n",
        "stands for **Visual Geometry Group** from Oxford\n",
        "\n",
        "##Google Inception-V3\n",
        "https://arxiv.org/pdf/1409.4842.pdf\n",
        "\n",
        "##Microsoft ResNet-50\n",
        "https://arxiv.org/pdf/1512.03385.pdf\n",
        " \n",
        "##SqueezeNet\n",
        "https://arxiv.org/pdf/1602.07360.pdf\n",
        "\n",
        "##Spatial Transformer Networks\n",
        "https://arxiv.org/pdf/1506.02025.pdf\n",
        "\n",
        "##DenseNet\n",
        "extension of ResNet\n",
        "\n",
        "https://arxiv.org/pdf/1608.06993.pdf"
      ]
    },
    {
      "metadata": {
        "id": "0x0VBN4MSLuK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Model for cats vs dogs\n",
        "Prepare and train a model to predict cats versus dogs"
      ]
    },
    {
      "metadata": {
        "id": "LVwkKxKeSl9_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Prepare dataset"
      ]
    },
    {
      "metadata": {
        "id": "KiyyI1hdSg2y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XR4V82aCSqt6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "work_dir = './data/ch2/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rSS3plxOrxGL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17034
        },
        "outputId": "1287ee04-11af-48e4-ce29-a5c5de25e23c"
      },
      "cell_type": "code",
      "source": [
        "image_names = sorted(os.listdir(os.path.join(work_dir,'train')))\n",
        "image_names\n",
        "# make sure the filenames are correct"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cat.0.jpg',\n",
              " 'cat.1.jpg',\n",
              " 'cat.10.jpg',\n",
              " 'cat.100.jpg',\n",
              " 'cat.1000.jpg',\n",
              " 'cat.10000.jpg',\n",
              " 'cat.10001.jpg',\n",
              " 'cat.10002.jpg',\n",
              " 'cat.10003.jpg',\n",
              " 'cat.10004.jpg',\n",
              " 'cat.10005.jpg',\n",
              " 'cat.10006.jpg',\n",
              " 'cat.10007.jpg',\n",
              " 'cat.10008.jpg',\n",
              " 'cat.10009.jpg',\n",
              " 'cat.1001.jpg',\n",
              " 'cat.10010.jpg',\n",
              " 'cat.10011.jpg',\n",
              " 'cat.10012.jpg',\n",
              " 'cat.10013.jpg',\n",
              " 'cat.10014.jpg',\n",
              " 'cat.10015.jpg',\n",
              " 'cat.10016.jpg',\n",
              " 'cat.10017.jpg',\n",
              " 'cat.10018.jpg',\n",
              " 'cat.10019.jpg',\n",
              " 'cat.1002.jpg',\n",
              " 'cat.10020.jpg',\n",
              " 'cat.10021.jpg',\n",
              " 'cat.10022.jpg',\n",
              " 'cat.10023.jpg',\n",
              " 'cat.10024.jpg',\n",
              " 'cat.10025.jpg',\n",
              " 'cat.10026.jpg',\n",
              " 'cat.10027.jpg',\n",
              " 'cat.10028.jpg',\n",
              " 'cat.10029.jpg',\n",
              " 'cat.1003.jpg',\n",
              " 'cat.10030.jpg',\n",
              " 'cat.10031.jpg',\n",
              " 'cat.10032.jpg',\n",
              " 'cat.10033.jpg',\n",
              " 'cat.10034.jpg',\n",
              " 'cat.10035.jpg',\n",
              " 'cat.10036.jpg',\n",
              " 'cat.10037.jpg',\n",
              " 'cat.10038.jpg',\n",
              " 'cat.10039.jpg',\n",
              " 'cat.1004.jpg',\n",
              " 'cat.10040.jpg',\n",
              " 'cat.10041.jpg',\n",
              " 'cat.10042.jpg',\n",
              " 'cat.10043.jpg',\n",
              " 'cat.10044.jpg',\n",
              " 'cat.10045.jpg',\n",
              " 'cat.10046.jpg',\n",
              " 'cat.10047.jpg',\n",
              " 'cat.10048.jpg',\n",
              " 'cat.10049.jpg',\n",
              " 'cat.1005.jpg',\n",
              " 'cat.10050.jpg',\n",
              " 'cat.10051.jpg',\n",
              " 'cat.10052.jpg',\n",
              " 'cat.10053.jpg',\n",
              " 'cat.10054.jpg',\n",
              " 'cat.10055.jpg',\n",
              " 'cat.10056.jpg',\n",
              " 'cat.10057.jpg',\n",
              " 'cat.10058.jpg',\n",
              " 'cat.10059.jpg',\n",
              " 'cat.1006.jpg',\n",
              " 'cat.10060.jpg',\n",
              " 'cat.10061.jpg',\n",
              " 'cat.10062.jpg',\n",
              " 'cat.10063.jpg',\n",
              " 'cat.10064.jpg',\n",
              " 'cat.10065.jpg',\n",
              " 'cat.10066.jpg',\n",
              " 'cat.10067.jpg',\n",
              " 'cat.10068.jpg',\n",
              " 'cat.10069.jpg',\n",
              " 'cat.1007.jpg',\n",
              " 'cat.10070.jpg',\n",
              " 'cat.10071.jpg',\n",
              " 'cat.10072.jpg',\n",
              " 'cat.10073.jpg',\n",
              " 'cat.10074.jpg',\n",
              " 'cat.10075.jpg',\n",
              " 'cat.10076.jpg',\n",
              " 'cat.10077.jpg',\n",
              " 'cat.10078.jpg',\n",
              " 'cat.10079.jpg',\n",
              " 'cat.1008.jpg',\n",
              " 'cat.10080.jpg',\n",
              " 'cat.10081.jpg',\n",
              " 'cat.10082.jpg',\n",
              " 'cat.10083.jpg',\n",
              " 'cat.10084.jpg',\n",
              " 'cat.10085.jpg',\n",
              " 'cat.10086.jpg',\n",
              " 'cat.10087.jpg',\n",
              " 'cat.10088.jpg',\n",
              " 'cat.10089.jpg',\n",
              " 'cat.1009.jpg',\n",
              " 'cat.10090.jpg',\n",
              " 'cat.10091.jpg',\n",
              " 'cat.10092.jpg',\n",
              " 'cat.10093.jpg',\n",
              " 'cat.10094.jpg',\n",
              " 'cat.10095.jpg',\n",
              " 'cat.10096.jpg',\n",
              " 'cat.10097.jpg',\n",
              " 'cat.10098.jpg',\n",
              " 'cat.10099.jpg',\n",
              " 'cat.101.jpg',\n",
              " 'cat.1010.jpg',\n",
              " 'cat.10100.jpg',\n",
              " 'cat.10101.jpg',\n",
              " 'cat.10102.jpg',\n",
              " 'cat.10103.jpg',\n",
              " 'cat.10104.jpg',\n",
              " 'cat.10105.jpg',\n",
              " 'cat.10106.jpg',\n",
              " 'cat.10107.jpg',\n",
              " 'cat.10108.jpg',\n",
              " 'cat.10109.jpg',\n",
              " 'cat.1011.jpg',\n",
              " 'cat.10110.jpg',\n",
              " 'cat.10111.jpg',\n",
              " 'cat.10112.jpg',\n",
              " 'cat.10113.jpg',\n",
              " 'cat.10114.jpg',\n",
              " 'cat.10115.jpg',\n",
              " 'cat.10116.jpg',\n",
              " 'cat.10117.jpg',\n",
              " 'cat.10118.jpg',\n",
              " 'cat.10119.jpg',\n",
              " 'cat.1012.jpg',\n",
              " 'cat.10120.jpg',\n",
              " 'cat.10121.jpg',\n",
              " 'cat.10122.jpg',\n",
              " 'cat.10123.jpg',\n",
              " 'cat.10124.jpg',\n",
              " 'cat.10125.jpg',\n",
              " 'cat.10126.jpg',\n",
              " 'cat.10127.jpg',\n",
              " 'cat.10128.jpg',\n",
              " 'cat.10129.jpg',\n",
              " 'cat.1013.jpg',\n",
              " 'cat.10130.jpg',\n",
              " 'cat.10131.jpg',\n",
              " 'cat.10132.jpg',\n",
              " 'cat.10133.jpg',\n",
              " 'cat.10134.jpg',\n",
              " 'cat.10135.jpg',\n",
              " 'cat.10136.jpg',\n",
              " 'cat.10137.jpg',\n",
              " 'cat.10138.jpg',\n",
              " 'cat.10139.jpg',\n",
              " 'cat.1014.jpg',\n",
              " 'cat.10140.jpg',\n",
              " 'cat.10141.jpg',\n",
              " 'cat.10142.jpg',\n",
              " 'cat.10143.jpg',\n",
              " 'cat.10144.jpg',\n",
              " 'cat.10145.jpg',\n",
              " 'cat.10146.jpg',\n",
              " 'cat.10147.jpg',\n",
              " 'cat.10148.jpg',\n",
              " 'cat.10149.jpg',\n",
              " 'cat.1015.jpg',\n",
              " 'cat.10150.jpg',\n",
              " 'cat.10151.jpg',\n",
              " 'cat.10152.jpg',\n",
              " 'cat.10153.jpg',\n",
              " 'cat.10154.jpg',\n",
              " 'cat.10155.jpg',\n",
              " 'cat.10156.jpg',\n",
              " 'cat.10157.jpg',\n",
              " 'cat.10158.jpg',\n",
              " 'cat.10159.jpg',\n",
              " 'cat.1016.jpg',\n",
              " 'cat.10160.jpg',\n",
              " 'cat.10161.jpg',\n",
              " 'cat.10162.jpg',\n",
              " 'cat.10163.jpg',\n",
              " 'cat.10164.jpg',\n",
              " 'cat.10165.jpg',\n",
              " 'cat.10166.jpg',\n",
              " 'cat.10167.jpg',\n",
              " 'cat.10168.jpg',\n",
              " 'cat.10169.jpg',\n",
              " 'cat.1017.jpg',\n",
              " 'cat.10170.jpg',\n",
              " 'cat.10171.jpg',\n",
              " 'cat.10172.jpg',\n",
              " 'cat.10173.jpg',\n",
              " 'cat.10174.jpg',\n",
              " 'cat.10175.jpg',\n",
              " 'cat.10176.jpg',\n",
              " 'cat.10177.jpg',\n",
              " 'cat.10178.jpg',\n",
              " 'cat.10179.jpg',\n",
              " 'cat.1018.jpg',\n",
              " 'cat.10180.jpg',\n",
              " 'cat.10181.jpg',\n",
              " 'cat.10182.jpg',\n",
              " 'cat.10183.jpg',\n",
              " 'cat.10184.jpg',\n",
              " 'cat.10185.jpg',\n",
              " 'cat.10186.jpg',\n",
              " 'cat.10187.jpg',\n",
              " 'cat.10188.jpg',\n",
              " 'cat.10189.jpg',\n",
              " 'cat.1019.jpg',\n",
              " 'cat.10190.jpg',\n",
              " 'cat.10191.jpg',\n",
              " 'cat.10192.jpg',\n",
              " 'cat.10193.jpg',\n",
              " 'cat.10194.jpg',\n",
              " 'cat.10195.jpg',\n",
              " 'cat.10196.jpg',\n",
              " 'cat.10197.jpg',\n",
              " 'cat.10198.jpg',\n",
              " 'cat.10199.jpg',\n",
              " 'cat.102.jpg',\n",
              " 'cat.1020.jpg',\n",
              " 'cat.10200.jpg',\n",
              " 'cat.10201.jpg',\n",
              " 'cat.10202.jpg',\n",
              " 'cat.10203.jpg',\n",
              " 'cat.10204.jpg',\n",
              " 'cat.10205.jpg',\n",
              " 'cat.10206.jpg',\n",
              " 'cat.10207.jpg',\n",
              " 'cat.10208.jpg',\n",
              " 'cat.10209.jpg',\n",
              " 'cat.1021.jpg',\n",
              " 'cat.10210.jpg',\n",
              " 'cat.10211.jpg',\n",
              " 'cat.10212.jpg',\n",
              " 'cat.10213.jpg',\n",
              " 'cat.10214.jpg',\n",
              " 'cat.10215.jpg',\n",
              " 'cat.10216.jpg',\n",
              " 'cat.10217.jpg',\n",
              " 'cat.10218.jpg',\n",
              " 'cat.10219.jpg',\n",
              " 'cat.1022.jpg',\n",
              " 'cat.10220.jpg',\n",
              " 'cat.10221.jpg',\n",
              " 'cat.10222.jpg',\n",
              " 'cat.10223.jpg',\n",
              " 'cat.10224.jpg',\n",
              " 'cat.10225.jpg',\n",
              " 'cat.10226.jpg',\n",
              " 'cat.10227.jpg',\n",
              " 'cat.10228.jpg',\n",
              " 'cat.10229.jpg',\n",
              " 'cat.1023.jpg',\n",
              " 'cat.10230.jpg',\n",
              " 'cat.10231.jpg',\n",
              " 'cat.10232.jpg',\n",
              " 'cat.10233.jpg',\n",
              " 'cat.10234.jpg',\n",
              " 'cat.10235.jpg',\n",
              " 'cat.10236.jpg',\n",
              " 'cat.10237.jpg',\n",
              " 'cat.10238.jpg',\n",
              " 'cat.10239.jpg',\n",
              " 'cat.1024.jpg',\n",
              " 'cat.10240.jpg',\n",
              " 'cat.10241.jpg',\n",
              " 'cat.10242.jpg',\n",
              " 'cat.10243.jpg',\n",
              " 'cat.10244.jpg',\n",
              " 'cat.10245.jpg',\n",
              " 'cat.10246.jpg',\n",
              " 'cat.10247.jpg',\n",
              " 'cat.10248.jpg',\n",
              " 'cat.10249.jpg',\n",
              " 'cat.1025.jpg',\n",
              " 'cat.10250.jpg',\n",
              " 'cat.10251.jpg',\n",
              " 'cat.10252.jpg',\n",
              " 'cat.10253.jpg',\n",
              " 'cat.10254.jpg',\n",
              " 'cat.10255.jpg',\n",
              " 'cat.10256.jpg',\n",
              " 'cat.10257.jpg',\n",
              " 'cat.10258.jpg',\n",
              " 'cat.10259.jpg',\n",
              " 'cat.1026.jpg',\n",
              " 'cat.10260.jpg',\n",
              " 'cat.10261.jpg',\n",
              " 'cat.10262.jpg',\n",
              " 'cat.10263.jpg',\n",
              " 'cat.10264.jpg',\n",
              " 'cat.10265.jpg',\n",
              " 'cat.10266.jpg',\n",
              " 'cat.10267.jpg',\n",
              " 'cat.10268.jpg',\n",
              " 'cat.10269.jpg',\n",
              " 'cat.1027.jpg',\n",
              " 'cat.10270.jpg',\n",
              " 'cat.10271.jpg',\n",
              " 'cat.10272.jpg',\n",
              " 'cat.10273.jpg',\n",
              " 'cat.10274.jpg',\n",
              " 'cat.10275.jpg',\n",
              " 'cat.10276.jpg',\n",
              " 'cat.10277.jpg',\n",
              " 'cat.10278.jpg',\n",
              " 'cat.10279.jpg',\n",
              " 'cat.1028.jpg',\n",
              " 'cat.10280.jpg',\n",
              " 'cat.10281.jpg',\n",
              " 'cat.10282.jpg',\n",
              " 'cat.10283.jpg',\n",
              " 'cat.10284.jpg',\n",
              " 'cat.10285.jpg',\n",
              " 'cat.10286.jpg',\n",
              " 'cat.10287.jpg',\n",
              " 'cat.10288.jpg',\n",
              " 'cat.10289.jpg',\n",
              " 'cat.1029.jpg',\n",
              " 'cat.10290.jpg',\n",
              " 'cat.10291.jpg',\n",
              " 'cat.10292.jpg',\n",
              " 'cat.10293.jpg',\n",
              " 'cat.10294.jpg',\n",
              " 'cat.10295.jpg',\n",
              " 'cat.10296.jpg',\n",
              " 'cat.10297.jpg',\n",
              " 'cat.10298.jpg',\n",
              " 'cat.10299.jpg',\n",
              " 'cat.103.jpg',\n",
              " 'cat.1030.jpg',\n",
              " 'cat.10300.jpg',\n",
              " 'cat.10301.jpg',\n",
              " 'cat.10302.jpg',\n",
              " 'cat.10303.jpg',\n",
              " 'cat.10304.jpg',\n",
              " 'cat.10305.jpg',\n",
              " 'cat.10306.jpg',\n",
              " 'cat.10307.jpg',\n",
              " 'cat.10308.jpg',\n",
              " 'cat.10309.jpg',\n",
              " 'cat.1031.jpg',\n",
              " 'cat.10310.jpg',\n",
              " 'cat.10311.jpg',\n",
              " 'cat.10312.jpg',\n",
              " 'cat.10313.jpg',\n",
              " 'cat.10314.jpg',\n",
              " 'cat.10315.jpg',\n",
              " 'cat.10316.jpg',\n",
              " 'cat.10317.jpg',\n",
              " 'cat.10318.jpg',\n",
              " 'cat.10319.jpg',\n",
              " 'cat.1032.jpg',\n",
              " 'cat.10320.jpg',\n",
              " 'cat.10321.jpg',\n",
              " 'cat.10322.jpg',\n",
              " 'cat.10323.jpg',\n",
              " 'cat.10324.jpg',\n",
              " 'cat.10325.jpg',\n",
              " 'cat.10326.jpg',\n",
              " 'cat.10327.jpg',\n",
              " 'cat.10328.jpg',\n",
              " 'cat.10329.jpg',\n",
              " 'cat.1033.jpg',\n",
              " 'cat.10330.jpg',\n",
              " 'cat.10331.jpg',\n",
              " 'cat.10332.jpg',\n",
              " 'cat.10333.jpg',\n",
              " 'cat.10334.jpg',\n",
              " 'cat.10335.jpg',\n",
              " 'cat.10336.jpg',\n",
              " 'cat.10337.jpg',\n",
              " 'cat.10338.jpg',\n",
              " 'cat.10339.jpg',\n",
              " 'cat.1034.jpg',\n",
              " 'cat.10340.jpg',\n",
              " 'cat.10341.jpg',\n",
              " 'cat.10342.jpg',\n",
              " 'cat.10343.jpg',\n",
              " 'cat.10344.jpg',\n",
              " 'cat.10345.jpg',\n",
              " 'cat.10346.jpg',\n",
              " 'cat.10347.jpg',\n",
              " 'cat.10348.jpg',\n",
              " 'cat.10349.jpg',\n",
              " 'cat.1035.jpg',\n",
              " 'cat.10350.jpg',\n",
              " 'cat.10351.jpg',\n",
              " 'cat.10352.jpg',\n",
              " 'cat.10353.jpg',\n",
              " 'cat.10354.jpg',\n",
              " 'cat.10355.jpg',\n",
              " 'cat.10356.jpg',\n",
              " 'cat.10357.jpg',\n",
              " 'cat.10358.jpg',\n",
              " 'cat.10359.jpg',\n",
              " 'cat.1036.jpg',\n",
              " 'cat.10360.jpg',\n",
              " 'cat.10361.jpg',\n",
              " 'cat.10362.jpg',\n",
              " 'cat.10363.jpg',\n",
              " 'cat.10364.jpg',\n",
              " 'cat.10365.jpg',\n",
              " 'cat.10366.jpg',\n",
              " 'cat.10367.jpg',\n",
              " 'cat.10368.jpg',\n",
              " 'cat.10369.jpg',\n",
              " 'cat.1037.jpg',\n",
              " 'cat.10370.jpg',\n",
              " 'cat.10371.jpg',\n",
              " 'cat.10372.jpg',\n",
              " 'cat.10373.jpg',\n",
              " 'cat.10374.jpg',\n",
              " 'cat.10375.jpg',\n",
              " 'cat.10376.jpg',\n",
              " 'cat.10377.jpg',\n",
              " 'cat.10378.jpg',\n",
              " 'cat.10379.jpg',\n",
              " 'cat.1038.jpg',\n",
              " 'cat.10380.jpg',\n",
              " 'cat.10381.jpg',\n",
              " 'cat.10382.jpg',\n",
              " 'cat.10383.jpg',\n",
              " 'cat.10384.jpg',\n",
              " 'cat.10385.jpg',\n",
              " 'cat.10386.jpg',\n",
              " 'cat.10387.jpg',\n",
              " 'cat.10388.jpg',\n",
              " 'cat.10389.jpg',\n",
              " 'cat.1039.jpg',\n",
              " 'cat.10390.jpg',\n",
              " 'cat.10391.jpg',\n",
              " 'cat.10392.jpg',\n",
              " 'cat.10393.jpg',\n",
              " 'cat.10394.jpg',\n",
              " 'cat.10395.jpg',\n",
              " 'cat.10396.jpg',\n",
              " 'cat.10397.jpg',\n",
              " 'cat.10398.jpg',\n",
              " 'cat.10399.jpg',\n",
              " 'cat.104.jpg',\n",
              " 'cat.1040.jpg',\n",
              " 'cat.10400.jpg',\n",
              " 'cat.10401.jpg',\n",
              " 'cat.10402.jpg',\n",
              " 'cat.10403.jpg',\n",
              " 'cat.10404.jpg',\n",
              " 'cat.10405.jpg',\n",
              " 'cat.10406.jpg',\n",
              " 'cat.10407.jpg',\n",
              " 'cat.10408.jpg',\n",
              " 'cat.10409.jpg',\n",
              " 'cat.1041.jpg',\n",
              " 'cat.10410.jpg',\n",
              " 'cat.10411.jpg',\n",
              " 'cat.10412.jpg',\n",
              " 'cat.10413.jpg',\n",
              " 'cat.10414.jpg',\n",
              " 'cat.10415.jpg',\n",
              " 'cat.10416.jpg',\n",
              " 'cat.10417.jpg',\n",
              " 'cat.10418.jpg',\n",
              " 'cat.10419.jpg',\n",
              " 'cat.1042.jpg',\n",
              " 'cat.10420.jpg',\n",
              " 'cat.10421.jpg',\n",
              " 'cat.10422.jpg',\n",
              " 'cat.10423.jpg',\n",
              " 'cat.10424.jpg',\n",
              " 'cat.10425.jpg',\n",
              " 'cat.10426.jpg',\n",
              " 'cat.10427.jpg',\n",
              " 'cat.10428.jpg',\n",
              " 'cat.10429.jpg',\n",
              " 'cat.1043.jpg',\n",
              " 'cat.10430.jpg',\n",
              " 'cat.10431.jpg',\n",
              " 'cat.10432.jpg',\n",
              " 'cat.10433.jpg',\n",
              " 'cat.10434.jpg',\n",
              " 'cat.10435.jpg',\n",
              " 'cat.10436.jpg',\n",
              " 'cat.10437.jpg',\n",
              " 'cat.10438.jpg',\n",
              " 'cat.10439.jpg',\n",
              " 'cat.1044.jpg',\n",
              " 'cat.10440.jpg',\n",
              " 'cat.10441.jpg',\n",
              " 'cat.10442.jpg',\n",
              " 'cat.10443.jpg',\n",
              " 'cat.10444.jpg',\n",
              " 'cat.10445.jpg',\n",
              " 'cat.10446.jpg',\n",
              " 'cat.10447.jpg',\n",
              " 'cat.10448.jpg',\n",
              " 'cat.10449.jpg',\n",
              " 'cat.1045.jpg',\n",
              " 'cat.10450.jpg',\n",
              " 'cat.10451.jpg',\n",
              " 'cat.10452.jpg',\n",
              " 'cat.10453.jpg',\n",
              " 'cat.10454.jpg',\n",
              " 'cat.10455.jpg',\n",
              " 'cat.10456.jpg',\n",
              " 'cat.10457.jpg',\n",
              " 'cat.10458.jpg',\n",
              " 'cat.10459.jpg',\n",
              " 'cat.1046.jpg',\n",
              " 'cat.10460.jpg',\n",
              " 'cat.10461.jpg',\n",
              " 'cat.10462.jpg',\n",
              " 'cat.10463.jpg',\n",
              " 'cat.10464.jpg',\n",
              " 'cat.10465.jpg',\n",
              " 'cat.10466.jpg',\n",
              " 'cat.10467.jpg',\n",
              " 'cat.10468.jpg',\n",
              " 'cat.10469.jpg',\n",
              " 'cat.1047.jpg',\n",
              " 'cat.10470.jpg',\n",
              " 'cat.10471.jpg',\n",
              " 'cat.10472.jpg',\n",
              " 'cat.10473.jpg',\n",
              " 'cat.10474.jpg',\n",
              " 'cat.10475.jpg',\n",
              " 'cat.10476.jpg',\n",
              " 'cat.10477.jpg',\n",
              " 'cat.10478.jpg',\n",
              " 'cat.10479.jpg',\n",
              " 'cat.1048.jpg',\n",
              " 'cat.10480.jpg',\n",
              " 'cat.10481.jpg',\n",
              " 'cat.10482.jpg',\n",
              " 'cat.10483.jpg',\n",
              " 'cat.10484.jpg',\n",
              " 'cat.10485.jpg',\n",
              " 'cat.10486.jpg',\n",
              " 'cat.10487.jpg',\n",
              " 'cat.10488.jpg',\n",
              " 'cat.10489.jpg',\n",
              " 'cat.1049.jpg',\n",
              " 'cat.10490.jpg',\n",
              " 'cat.10491.jpg',\n",
              " 'cat.10492.jpg',\n",
              " 'cat.10493.jpg',\n",
              " 'cat.10494.jpg',\n",
              " 'cat.10495.jpg',\n",
              " 'cat.10496.jpg',\n",
              " 'cat.10497.jpg',\n",
              " 'cat.10498.jpg',\n",
              " 'cat.10499.jpg',\n",
              " 'cat.105.jpg',\n",
              " 'cat.1050.jpg',\n",
              " 'cat.10500.jpg',\n",
              " 'cat.10501.jpg',\n",
              " 'cat.10502.jpg',\n",
              " 'cat.10503.jpg',\n",
              " 'cat.10504.jpg',\n",
              " 'cat.10505.jpg',\n",
              " 'cat.10506.jpg',\n",
              " 'cat.10507.jpg',\n",
              " 'cat.10508.jpg',\n",
              " 'cat.10509.jpg',\n",
              " 'cat.1051.jpg',\n",
              " 'cat.10510.jpg',\n",
              " 'cat.10511.jpg',\n",
              " 'cat.10512.jpg',\n",
              " 'cat.10513.jpg',\n",
              " 'cat.10514.jpg',\n",
              " 'cat.10515.jpg',\n",
              " 'cat.10516.jpg',\n",
              " 'cat.10517.jpg',\n",
              " 'cat.10518.jpg',\n",
              " 'cat.10519.jpg',\n",
              " 'cat.1052.jpg',\n",
              " 'cat.10520.jpg',\n",
              " 'cat.10521.jpg',\n",
              " 'cat.10522.jpg',\n",
              " 'cat.10523.jpg',\n",
              " 'cat.10524.jpg',\n",
              " 'cat.10525.jpg',\n",
              " 'cat.10526.jpg',\n",
              " 'cat.10527.jpg',\n",
              " 'cat.10528.jpg',\n",
              " 'cat.10529.jpg',\n",
              " 'cat.1053.jpg',\n",
              " 'cat.10530.jpg',\n",
              " 'cat.10531.jpg',\n",
              " 'cat.10532.jpg',\n",
              " 'cat.10533.jpg',\n",
              " 'cat.10534.jpg',\n",
              " 'cat.10535.jpg',\n",
              " 'cat.10536.jpg',\n",
              " 'cat.10537.jpg',\n",
              " 'cat.10538.jpg',\n",
              " 'cat.10539.jpg',\n",
              " 'cat.1054.jpg',\n",
              " 'cat.10540.jpg',\n",
              " 'cat.10541.jpg',\n",
              " 'cat.10542.jpg',\n",
              " 'cat.10543.jpg',\n",
              " 'cat.10544.jpg',\n",
              " 'cat.10545.jpg',\n",
              " 'cat.10546.jpg',\n",
              " 'cat.10547.jpg',\n",
              " 'cat.10548.jpg',\n",
              " 'cat.10549.jpg',\n",
              " 'cat.1055.jpg',\n",
              " 'cat.10550.jpg',\n",
              " 'cat.10551.jpg',\n",
              " 'cat.10552.jpg',\n",
              " 'cat.10553.jpg',\n",
              " 'cat.10554.jpg',\n",
              " 'cat.10555.jpg',\n",
              " 'cat.10556.jpg',\n",
              " 'cat.10557.jpg',\n",
              " 'cat.10558.jpg',\n",
              " 'cat.10559.jpg',\n",
              " 'cat.1056.jpg',\n",
              " 'cat.10560.jpg',\n",
              " 'cat.10561.jpg',\n",
              " 'cat.10562.jpg',\n",
              " 'cat.10563.jpg',\n",
              " 'cat.10564.jpg',\n",
              " 'cat.10565.jpg',\n",
              " 'cat.10566.jpg',\n",
              " 'cat.10567.jpg',\n",
              " 'cat.10568.jpg',\n",
              " 'cat.10569.jpg',\n",
              " 'cat.1057.jpg',\n",
              " 'cat.10570.jpg',\n",
              " 'cat.10571.jpg',\n",
              " 'cat.10572.jpg',\n",
              " 'cat.10573.jpg',\n",
              " 'cat.10574.jpg',\n",
              " 'cat.10575.jpg',\n",
              " 'cat.10576.jpg',\n",
              " 'cat.10577.jpg',\n",
              " 'cat.10578.jpg',\n",
              " 'cat.10579.jpg',\n",
              " 'cat.1058.jpg',\n",
              " 'cat.10580.jpg',\n",
              " 'cat.10581.jpg',\n",
              " 'cat.10582.jpg',\n",
              " 'cat.10583.jpg',\n",
              " 'cat.10584.jpg',\n",
              " 'cat.10585.jpg',\n",
              " 'cat.10586.jpg',\n",
              " 'cat.10587.jpg',\n",
              " 'cat.10588.jpg',\n",
              " 'cat.10589.jpg',\n",
              " 'cat.1059.jpg',\n",
              " 'cat.10590.jpg',\n",
              " 'cat.10591.jpg',\n",
              " 'cat.10592.jpg',\n",
              " 'cat.10593.jpg',\n",
              " 'cat.10594.jpg',\n",
              " 'cat.10595.jpg',\n",
              " 'cat.10596.jpg',\n",
              " 'cat.10597.jpg',\n",
              " 'cat.10598.jpg',\n",
              " 'cat.10599.jpg',\n",
              " 'cat.106.jpg',\n",
              " 'cat.1060.jpg',\n",
              " 'cat.10600.jpg',\n",
              " 'cat.10601.jpg',\n",
              " 'cat.10602.jpg',\n",
              " 'cat.10603.jpg',\n",
              " 'cat.10604.jpg',\n",
              " 'cat.10605.jpg',\n",
              " 'cat.10606.jpg',\n",
              " 'cat.10607.jpg',\n",
              " 'cat.10608.jpg',\n",
              " 'cat.10609.jpg',\n",
              " 'cat.1061.jpg',\n",
              " 'cat.10610.jpg',\n",
              " 'cat.10611.jpg',\n",
              " 'cat.10612.jpg',\n",
              " 'cat.10613.jpg',\n",
              " 'cat.10614.jpg',\n",
              " 'cat.10615.jpg',\n",
              " 'cat.10616.jpg',\n",
              " 'cat.10617.jpg',\n",
              " 'cat.10618.jpg',\n",
              " 'cat.10619.jpg',\n",
              " 'cat.1062.jpg',\n",
              " 'cat.10620.jpg',\n",
              " 'cat.10621.jpg',\n",
              " 'cat.10622.jpg',\n",
              " 'cat.10623.jpg',\n",
              " 'cat.10624.jpg',\n",
              " 'cat.10625.jpg',\n",
              " 'cat.10626.jpg',\n",
              " 'cat.10627.jpg',\n",
              " 'cat.10628.jpg',\n",
              " 'cat.10629.jpg',\n",
              " 'cat.1063.jpg',\n",
              " 'cat.10630.jpg',\n",
              " 'cat.10631.jpg',\n",
              " 'cat.10632.jpg',\n",
              " 'cat.10633.jpg',\n",
              " 'cat.10634.jpg',\n",
              " 'cat.10635.jpg',\n",
              " 'cat.10636.jpg',\n",
              " 'cat.10637.jpg',\n",
              " 'cat.10638.jpg',\n",
              " 'cat.10639.jpg',\n",
              " 'cat.1064.jpg',\n",
              " 'cat.10640.jpg',\n",
              " 'cat.10641.jpg',\n",
              " 'cat.10642.jpg',\n",
              " 'cat.10643.jpg',\n",
              " 'cat.10644.jpg',\n",
              " 'cat.10645.jpg',\n",
              " 'cat.10646.jpg',\n",
              " 'cat.10647.jpg',\n",
              " 'cat.10648.jpg',\n",
              " 'cat.10649.jpg',\n",
              " 'cat.1065.jpg',\n",
              " 'cat.10650.jpg',\n",
              " 'cat.10651.jpg',\n",
              " 'cat.10652.jpg',\n",
              " 'cat.10653.jpg',\n",
              " 'cat.10654.jpg',\n",
              " 'cat.10655.jpg',\n",
              " 'cat.10656.jpg',\n",
              " 'cat.10657.jpg',\n",
              " 'cat.10658.jpg',\n",
              " 'cat.10659.jpg',\n",
              " 'cat.1066.jpg',\n",
              " 'cat.10660.jpg',\n",
              " 'cat.10661.jpg',\n",
              " 'cat.10662.jpg',\n",
              " 'cat.10663.jpg',\n",
              " 'cat.10664.jpg',\n",
              " 'cat.10665.jpg',\n",
              " 'cat.10666.jpg',\n",
              " 'cat.10667.jpg',\n",
              " 'cat.10668.jpg',\n",
              " 'cat.10669.jpg',\n",
              " 'cat.1067.jpg',\n",
              " 'cat.10670.jpg',\n",
              " 'cat.10671.jpg',\n",
              " 'cat.10672.jpg',\n",
              " 'cat.10673.jpg',\n",
              " 'cat.10674.jpg',\n",
              " 'cat.10675.jpg',\n",
              " 'cat.10676.jpg',\n",
              " 'cat.10677.jpg',\n",
              " 'cat.10678.jpg',\n",
              " 'cat.10679.jpg',\n",
              " 'cat.1068.jpg',\n",
              " 'cat.10680.jpg',\n",
              " 'cat.10681.jpg',\n",
              " 'cat.10682.jpg',\n",
              " 'cat.10683.jpg',\n",
              " 'cat.10684.jpg',\n",
              " 'cat.10685.jpg',\n",
              " 'cat.10686.jpg',\n",
              " 'cat.10687.jpg',\n",
              " 'cat.10688.jpg',\n",
              " 'cat.10689.jpg',\n",
              " 'cat.1069.jpg',\n",
              " 'cat.10690.jpg',\n",
              " 'cat.10691.jpg',\n",
              " 'cat.10692.jpg',\n",
              " 'cat.10693.jpg',\n",
              " 'cat.10694.jpg',\n",
              " 'cat.10695.jpg',\n",
              " 'cat.10696.jpg',\n",
              " 'cat.10697.jpg',\n",
              " 'cat.10698.jpg',\n",
              " 'cat.10699.jpg',\n",
              " 'cat.107.jpg',\n",
              " 'cat.1070.jpg',\n",
              " 'cat.10700.jpg',\n",
              " 'cat.10701.jpg',\n",
              " 'cat.10702.jpg',\n",
              " 'cat.10703.jpg',\n",
              " 'cat.10704.jpg',\n",
              " 'cat.10705.jpg',\n",
              " 'cat.10706.jpg',\n",
              " 'cat.10707.jpg',\n",
              " 'cat.10708.jpg',\n",
              " 'cat.10709.jpg',\n",
              " 'cat.1071.jpg',\n",
              " 'cat.10710.jpg',\n",
              " 'cat.10711.jpg',\n",
              " 'cat.10712.jpg',\n",
              " 'cat.10713.jpg',\n",
              " 'cat.10714.jpg',\n",
              " 'cat.10715.jpg',\n",
              " 'cat.10716.jpg',\n",
              " 'cat.10717.jpg',\n",
              " 'cat.10718.jpg',\n",
              " 'cat.10719.jpg',\n",
              " 'cat.1072.jpg',\n",
              " 'cat.10720.jpg',\n",
              " 'cat.10721.jpg',\n",
              " 'cat.10722.jpg',\n",
              " 'cat.10723.jpg',\n",
              " 'cat.10724.jpg',\n",
              " 'cat.10725.jpg',\n",
              " 'cat.10726.jpg',\n",
              " 'cat.10727.jpg',\n",
              " 'cat.10728.jpg',\n",
              " 'cat.10729.jpg',\n",
              " 'cat.1073.jpg',\n",
              " 'cat.10730.jpg',\n",
              " 'cat.10731.jpg',\n",
              " 'cat.10732.jpg',\n",
              " 'cat.10733.jpg',\n",
              " 'cat.10734.jpg',\n",
              " 'cat.10735.jpg',\n",
              " 'cat.10736.jpg',\n",
              " 'cat.10737.jpg',\n",
              " 'cat.10738.jpg',\n",
              " 'cat.10739.jpg',\n",
              " 'cat.1074.jpg',\n",
              " 'cat.10740.jpg',\n",
              " 'cat.10741.jpg',\n",
              " 'cat.10742.jpg',\n",
              " 'cat.10743.jpg',\n",
              " 'cat.10744.jpg',\n",
              " 'cat.10745.jpg',\n",
              " 'cat.10746.jpg',\n",
              " 'cat.10747.jpg',\n",
              " 'cat.10748.jpg',\n",
              " 'cat.10749.jpg',\n",
              " 'cat.1075.jpg',\n",
              " 'cat.10750.jpg',\n",
              " 'cat.10751.jpg',\n",
              " 'cat.10752.jpg',\n",
              " 'cat.10753.jpg',\n",
              " 'cat.10754.jpg',\n",
              " 'cat.10755.jpg',\n",
              " 'cat.10756.jpg',\n",
              " 'cat.10757.jpg',\n",
              " 'cat.10758.jpg',\n",
              " 'cat.10759.jpg',\n",
              " 'cat.1076.jpg',\n",
              " 'cat.10760.jpg',\n",
              " 'cat.10761.jpg',\n",
              " 'cat.10762.jpg',\n",
              " 'cat.10763.jpg',\n",
              " 'cat.10764.jpg',\n",
              " 'cat.10765.jpg',\n",
              " 'cat.10766.jpg',\n",
              " 'cat.10767.jpg',\n",
              " 'cat.10768.jpg',\n",
              " 'cat.10769.jpg',\n",
              " 'cat.1077.jpg',\n",
              " 'cat.10770.jpg',\n",
              " 'cat.10771.jpg',\n",
              " 'cat.10772.jpg',\n",
              " 'cat.10773.jpg',\n",
              " 'cat.10774.jpg',\n",
              " 'cat.10775.jpg',\n",
              " 'cat.10776.jpg',\n",
              " 'cat.10777.jpg',\n",
              " 'cat.10778.jpg',\n",
              " 'cat.10779.jpg',\n",
              " 'cat.1078.jpg',\n",
              " 'cat.10780.jpg',\n",
              " 'cat.10781.jpg',\n",
              " 'cat.10782.jpg',\n",
              " 'cat.10783.jpg',\n",
              " 'cat.10784.jpg',\n",
              " 'cat.10785.jpg',\n",
              " 'cat.10786.jpg',\n",
              " 'cat.10787.jpg',\n",
              " 'cat.10788.jpg',\n",
              " 'cat.10789.jpg',\n",
              " 'cat.1079.jpg',\n",
              " 'cat.10790.jpg',\n",
              " 'cat.10791.jpg',\n",
              " 'cat.10792.jpg',\n",
              " 'cat.10793.jpg',\n",
              " 'cat.10794.jpg',\n",
              " 'cat.10795.jpg',\n",
              " 'cat.10796.jpg',\n",
              " 'cat.10797.jpg',\n",
              " 'cat.10798.jpg',\n",
              " 'cat.10799.jpg',\n",
              " 'cat.108.jpg',\n",
              " 'cat.1080.jpg',\n",
              " 'cat.10800.jpg',\n",
              " 'cat.10801.jpg',\n",
              " 'cat.10802.jpg',\n",
              " 'cat.10803.jpg',\n",
              " 'cat.10804.jpg',\n",
              " 'cat.10805.jpg',\n",
              " 'cat.10806.jpg',\n",
              " 'cat.10807.jpg',\n",
              " 'cat.10808.jpg',\n",
              " 'cat.10809.jpg',\n",
              " 'cat.1081.jpg',\n",
              " 'cat.10810.jpg',\n",
              " 'cat.10811.jpg',\n",
              " 'cat.10812.jpg',\n",
              " 'cat.10813.jpg',\n",
              " 'cat.10814.jpg',\n",
              " 'cat.10815.jpg',\n",
              " 'cat.10816.jpg',\n",
              " 'cat.10817.jpg',\n",
              " 'cat.10818.jpg',\n",
              " 'cat.10819.jpg',\n",
              " 'cat.1082.jpg',\n",
              " 'cat.10820.jpg',\n",
              " 'cat.10821.jpg',\n",
              " 'cat.10822.jpg',\n",
              " 'cat.10823.jpg',\n",
              " 'cat.10824.jpg',\n",
              " 'cat.10825.jpg',\n",
              " 'cat.10826.jpg',\n",
              " 'cat.10827.jpg',\n",
              " 'cat.10828.jpg',\n",
              " 'cat.10829.jpg',\n",
              " 'cat.1083.jpg',\n",
              " 'cat.10830.jpg',\n",
              " 'cat.10831.jpg',\n",
              " 'cat.10832.jpg',\n",
              " 'cat.10833.jpg',\n",
              " 'cat.10834.jpg',\n",
              " 'cat.10835.jpg',\n",
              " 'cat.10836.jpg',\n",
              " 'cat.10837.jpg',\n",
              " 'cat.10838.jpg',\n",
              " 'cat.10839.jpg',\n",
              " 'cat.1084.jpg',\n",
              " 'cat.10840.jpg',\n",
              " 'cat.10841.jpg',\n",
              " 'cat.10842.jpg',\n",
              " 'cat.10843.jpg',\n",
              " 'cat.10844.jpg',\n",
              " 'cat.10845.jpg',\n",
              " 'cat.10846.jpg',\n",
              " 'cat.10847.jpg',\n",
              " 'cat.10848.jpg',\n",
              " 'cat.10849.jpg',\n",
              " 'cat.1085.jpg',\n",
              " 'cat.10850.jpg',\n",
              " 'cat.10851.jpg',\n",
              " 'cat.10852.jpg',\n",
              " 'cat.10853.jpg',\n",
              " 'cat.10854.jpg',\n",
              " 'cat.10855.jpg',\n",
              " 'cat.10856.jpg',\n",
              " 'cat.10857.jpg',\n",
              " 'cat.10858.jpg',\n",
              " 'cat.10859.jpg',\n",
              " 'cat.1086.jpg',\n",
              " 'cat.10860.jpg',\n",
              " 'cat.10861.jpg',\n",
              " 'cat.10862.jpg',\n",
              " 'cat.10863.jpg',\n",
              " 'cat.10864.jpg',\n",
              " 'cat.10865.jpg',\n",
              " 'cat.10866.jpg',\n",
              " 'cat.10867.jpg',\n",
              " 'cat.10868.jpg',\n",
              " 'cat.10869.jpg',\n",
              " 'cat.1087.jpg',\n",
              " 'cat.10870.jpg',\n",
              " 'cat.10871.jpg',\n",
              " 'cat.10872.jpg',\n",
              " 'cat.10873.jpg',\n",
              " 'cat.10874.jpg',\n",
              " 'cat.10875.jpg',\n",
              " 'cat.10876.jpg',\n",
              " 'cat.10877.jpg',\n",
              " 'cat.10878.jpg',\n",
              " 'cat.10879.jpg',\n",
              " 'cat.1088.jpg',\n",
              " 'cat.10880.jpg',\n",
              " 'cat.10881.jpg',\n",
              " 'cat.10882.jpg',\n",
              " 'cat.10883.jpg',\n",
              " 'cat.10884.jpg',\n",
              " 'cat.10885.jpg',\n",
              " 'cat.10886.jpg',\n",
              " 'cat.10887.jpg',\n",
              " 'cat.10888.jpg',\n",
              " 'cat.10889.jpg',\n",
              " 'cat.1089.jpg',\n",
              " 'cat.10890.jpg',\n",
              " 'cat.10891.jpg',\n",
              " 'cat.10892.jpg',\n",
              " 'cat.10893.jpg',\n",
              " 'cat.10894.jpg',\n",
              " 'cat.10895.jpg',\n",
              " 'cat.10896.jpg',\n",
              " 'cat.10897.jpg',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "-cRG5ECJSzDy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def copy_files(prefix_str, range_start, range_end, target_dir):\n",
        "  image_paths = [os.path.join(work_dir, 'train', prefix_str + '.' + str(i) + '.jpg')\n",
        "                 for i in range(range_start, range_end)]\n",
        "  dest_dir = os.path.join(work_dir, 'data', target_dir, prefix_str)\n",
        "  os.makedirs(dest_dir)\n",
        "  for image_path in image_paths:\n",
        "    shutil.copy(image_path, dest_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tqX1YENksavo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this example, we will use only 1000 images of cats and dogs for training and 400 images of cats and dogs for testing, respectively."
      ]
    },
    {
      "metadata": {
        "id": "fCAyQke_r_sD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "copy_files('dog', 0, 1000, 'train')\n",
        "copy_files('cat', 0, 1000, 'train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xYA-vE04sKLJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "copy_files('dog', 1000, 1400, 'test')\n",
        "copy_files('cat', 1000, 1400, 'test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YsYJETdys6-V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Build a CNN Model"
      ]
    },
    {
      "metadata": {
        "id": "IrfHgOtKtCNO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fpEajHCPtG2-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "work_dir = './data/ch2/data/'\n",
        "image_height, image_width = 150, 150\n",
        "train_dir = os.path.join(work_dir, 'train')\n",
        "test_dir = os.path.join(work_dir, 'test')\n",
        "no_classes = 2\n",
        "no_validation = 800\n",
        "epochs = 2\n",
        "batch_size = 200\n",
        "no_train = 2000\n",
        "no_test = 800\n",
        "input_shape = (image_height, image_width, 3)\n",
        "epoch_steps = no_train // batch_size\n",
        "test_steps = no_test // batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IWM1gFrOt89L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reset graph\n",
        "tf.reset_default_graph()\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v5F_EHS0tzfQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def simple_cnn(input_shape):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Conv2D(filters=64,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   activation='relu',\n",
        "                                   input_shape=input_shape)\n",
        "           )\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(tf.keras.layers.Conv2D(filters=128,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   activation='relu')\n",
        "           )\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(units=1024, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(rate=0.4))\n",
        "  model.add(tf.keras.layers.Dense(units=no_classes, activation='softmax'))\n",
        "  \n",
        "  model.summary()\n",
        "  model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PjwPY6p0uXGW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "8abf8564-1605-48b8-c9ef-f9b9ad86c592"
      },
      "cell_type": "code",
      "source": [
        "simple_cnn_model = simple_cnn(input_shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 148, 148, 64)      1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 74, 74, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 72, 72, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 165888)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              169870336 \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 2050      \n",
            "=================================================================\n",
            "Total params: 169,948,034\n",
            "Trainable params: 169,948,034\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JReSHeA7ucRl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator_train = tf.keras.preprocessing.image.ImageDataGenerator(rescale= 1./255)\n",
        "generator_test = tf.keras.preprocessing.image.ImageDataGenerator(rescale= 1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B_TW9O0eucQH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c2b8b41e-b61b-4b8e-8355-a2d4ed66f52a"
      },
      "cell_type": "code",
      "source": [
        "train_images = generator_train.flow_from_directory(train_dir,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   target_size=(image_width, image_height))\n",
        "test_images = generator_test.flow_from_directory(test_dir,\n",
        "                                                 batch_size=batch_size,\n",
        "                                                 target_size=(image_width, image_height))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 800 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZZM_VOaSu9kn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "87e2d28d-b944-4915-92e3-3c4f3ba35409"
      },
      "cell_type": "code",
      "source": [
        "simple_cnn_model.fit_generator(train_images,\n",
        "                               steps_per_epoch=epoch_steps,\n",
        "                               epochs=epochs,\n",
        "                               validation_data=test_images,\n",
        "                               validation_steps=test_steps)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "10/10 [==============================] - 26s 3s/step - loss: 6.8504 - acc: 0.4890 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 2/2\n",
            "10/10 [==============================] - 15s 1s/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3568cfb358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "l679JphbvQMd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This model fits the data from the generator of training images. The\n",
        "number of epochs is defined from training, and validation data is\n",
        "passed for getting the performance of the model overtraining. This\n",
        "`fit_generator` enables parallel processing of data and model training.\n",
        "The CPU performs the rescaling while the GPU can perform the\n",
        "model training. This gives the high efficiency of computing resources."
      ]
    },
    {
      "metadata": {
        "id": "c-CKddRFvUmk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Augment the Dataset\n",
        "Data augmentation gives ways to increase the size of the dataset. Data\n",
        "augmentation introduces noise during training, producing robustness in the\n",
        "model to various inputs. This technique is useful in scenarios when the dataset is\n",
        "small and can be combined and used with other techniques."
      ]
    },
    {
      "metadata": {
        "id": "rCQ7vEw6v_Tg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator_train = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
        "                                                                  horizontal_flip=True,\n",
        "                                                                  zoom_range=0.3,\n",
        "                                                                  shear_range=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AuNEg8vTwQOj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This will change the zoom range, and shear effect of the training images and also flip the training images horizontally."
      ]
    },
    {
      "metadata": {
        "id": "xFZdTF52wdZs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator_test = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WtRAZCuIwfzZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "47fcfc6a-ae6c-477e-be27-47a3a490e43f"
      },
      "cell_type": "code",
      "source": [
        "train_images = generator_train.flow_from_directory(train_dir,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   target_size=(image_width,image_height))\n",
        "test_images = generator_test.flow_from_directory(test_dir,\n",
        "                                                 batch_size=batch_size,\n",
        "                                                 target_size=(image_width,image_height))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 800 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w--ERNuqwww_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "048775b4-595b-47e5-e254-e634c6ba3866"
      },
      "cell_type": "code",
      "source": [
        "simple_cnn_model.fit_generator(train_images,\n",
        "                               steps_per_epoch=epoch_steps,\n",
        "                               epochs=epochs,\n",
        "                               validation_data=test_images,\n",
        "                               validation_steps=test_steps)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "10/10 [==============================] - 28s 3s/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 2/2\n",
            "10/10 [==============================] - 25s 2s/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f356871f9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "OfNQmMN6w5vS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice how the accuracy increases as we apply data augmentation here."
      ]
    },
    {
      "metadata": {
        "id": "ldgfBqPnxKwr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Transfer Learning or Fine-Tuning of a Model\n",
        "Transfer learning is the process of learning from a pre-trained model that was\n",
        "trained on a larger dataset. Training a model with random initialization often\n",
        "takes time and energy to get the result. Initializing the model with a pre-trained\n",
        "model gives faster convergence, saving time and energy. These models that are\n",
        "pre-trained are often trained with carefully chosen hyperparameters.\n",
        "\n",
        "Either the several layers of the pre-trained model can be used without any\n",
        "modification, or can be bit trained to adapt to the changes."
      ]
    },
    {
      "metadata": {
        "id": "prSR3LhqxmF_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reset graph\n",
        "tf.reset_default_graph()\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HXAwtn6wxm4Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NRla0-IPyebS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.applications.VGG16(include_top=False)\n",
        "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XkaYYbIdzGIi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4a78b24-c78d-46be-f701-e8e55e83dcf4"
      },
      "cell_type": "code",
      "source": [
        "train_images = generator.flow_from_directory(train_dir,\n",
        "                                             batch_size=batch_size,\n",
        "                                             target_size=(image_width,image_height),\n",
        "                                             class_mode=None,\n",
        "                                             shuffle=False)\n",
        "train_bottleneck_features = model.predict_generator(train_images, epoch_steps)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dPyxfKYDzbxz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6cd3e9d9-2d74-478f-d6e5-8408684223f5"
      },
      "cell_type": "code",
      "source": [
        "test_images = generator.flow_from_directory(test_dir,\n",
        "                                            batch_size=batch_size,\n",
        "                                            target_size=(image_width,image_height),\n",
        "                                            class_mode=None,\n",
        "                                            shuffle=False)\n",
        "test_bottleneck_features = model.predict_generator(test_images, test_steps)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 800 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3dtvck210IQy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_labels = np.array([0] * int(no_train / 2) + [1] * int(no_train / 2))\n",
        "test_labels = np.array([0] * int(no_test / 2) + [1] * int(no_test / 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b_SHQrG00nlC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=train_bottleneck_features.shape[1:]))\n",
        "model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Dense(1, activation='softmax'))\n",
        "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UUHpDCdh1Ldq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "dde0ef1b-e02d-4389-f6c1-a7fd131bf924"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              8389632   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 8,390,657\n",
            "Trainable params: 8,390,657\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FFoh3O7t1VxI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "04b03c8e-544a-4a89-fe88-b554567f56a1"
      },
      "cell_type": "code",
      "source": [
        "model.fit(train_bottleneck_features,\n",
        "          train_labels,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(test_bottleneck_features, test_labels))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2000 samples, validate on 800 samples\n",
            "Epoch 1/2\n",
            "2000/2000 [==============================] - 1s 483us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 2/2\n",
            "2000/2000 [==============================] - 0s 108us/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f35675e5438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "mBfRJlr02Ewm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This gives a different approach to training the model and is useful\n",
        "when the training data is low. This is often a faster method to train a\n",
        "model. Only the final activations of the pre-trained model are used to\n",
        "adapt to the new task."
      ]
    },
    {
      "metadata": {
        "id": "XwSSNy_p3LLh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Fine-Tune Several Layers in Deep Learning\n",
        "A pre-trained model can be loaded and only a few layers can be trained. This\n",
        "approach works better when the given problem is very different from the images\n",
        "that the model is trained upon. **Fine-tuning** is a common practice in deep\n",
        "learning. This gives advantages when the dataset is smaller. The optimization\n",
        "also can be obtained faster.\n",
        "\n",
        "Training a deep network on a small dataset results in overfitting. This kind of\n",
        "overfitting can also be avoided using the fine-tuning procedure. The model\n",
        "trained on a bigger dataset should be also similar, as we are hoping that the\n",
        "activations and features are similar to the smaller dataset."
      ]
    },
    {
      "metadata": {
        "id": "IY09jhj-6Ohx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Choose the right model\n",
        "There are a lot of options for architectures. Based on the flexibility of\n",
        "deployment, you can choose the model. Remember that convolution is smaller\n",
        "and slower, but dense layers are bigger and faster. There is a trade-off between\n",
        "size, runtime, and accuracy. It is advisable to test out all the architectures before\n",
        "the final decision. Some models may work better than others, based on the\n",
        "application. You can reduce the input size to make the inference faster."
      ]
    },
    {
      "metadata": {
        "id": "h-JL5UmL6RzQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Underfitting and Overfitting\n",
        "The model may be sometimes too big or too small for the problem. This could be\n",
        "classified as underfitting or overfitting, respectively. Underfitting happens when\n",
        "the model is too small and can be measured when training accuracy is less.\n",
        "Overfitting happens when the model is too big and there is a large gap between\n",
        "training and testing accuracies.\n",
        "\n",
        "Underfitting can be solved by the following methods:\n",
        "* Getting more data\n",
        "* Trying out a bigger model\n",
        "* If the data is small, try transfer learning techniques or do data augmentation\n",
        "\n",
        "Overfitting can be solved by the following methods:\n",
        "* Regularizing using techniques such as dropout and batch normalization\n",
        "* Augmenting the dataset"
      ]
    },
    {
      "metadata": {
        "id": "gfNcLzc66hFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Gender and Age Detection from Face\n",
        "Applications may require gender and age detection from a face. The face image\n",
        "can be obtained by face detectors. The cropped images of faces can be supplied\n",
        "as training data, and the similar cropped face should be given for inference.\n",
        "Based on the required inference time, OpenCV, or CNN face detectors can be\n",
        "selected. For training, Inception or ResNet can be used. If the required inference\n",
        "time is much less because it is a video, it's better to use three convolutions\n",
        "followed by two fully connected layers. Note that there is usually a huge class\n",
        "imbalance in age datasets, hence using a different metric like precision and recall\n",
        "will be helpful."
      ]
    },
    {
      "metadata": {
        "id": "wJ61TbWK6tJQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Apparel Models\n",
        "Fine-tuning of apparel models is a good choice. Having multiple softmax layers\n",
        "that classify attributes will be useful here. The attributes could be a pattern,\n",
        "color, and so"
      ]
    },
    {
      "metadata": {
        "id": "WQoZH80d6xuJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Brand Safety\n",
        "Training bottleneck layers with Support Vector Machine (SVM) is a good\n",
        "option as the images can be quite different among classes. This is typically used\n",
        "for content moderation to help avoid images that are explicit."
      ]
    }
  ]
}