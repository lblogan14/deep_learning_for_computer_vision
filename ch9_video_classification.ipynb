{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch9_video_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/deep_learning_for_computer_vision/blob/master/ch9_video_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cHrM6RUE90sc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Understand and Classify Videos\n",
        "A video is nothing but a series of images. Video brings a new dimension to the\n",
        "image along the temporal direction. The spatial features of the images and temporal features of the video can be put together, providing a better outcome than just the image. The extra dimension also results in a lot of space and hence increases the complexity of training and inference. The computational demands are extremely high for processing a video. Video also changes the architecture of\n",
        "deep learning models as we have to consider the temporal features.\n",
        "\n",
        "Video classification is the task of labeling a video with a category. A category\n",
        "can be on the frame level or for the whole video. There could be actions or tasks\n",
        "performed in the video. Hence, a video classification may label the objects\n",
        "present in the video or label the actions happening in the video."
      ]
    },
    {
      "metadata": {
        "id": "P_nKa1Mn-iHC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Video Classification Datasets"
      ]
    },
    {
      "metadata": {
        "id": "4zhx06hY-8pn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##UCF101\n",
        "is used for action recognition.\n",
        "\n",
        "The videos are collected on YouTube and consist of realistic actions.\n",
        "There are 101 action categories available in this dataset. There is another dataset\n",
        "called UCF50 which has 50 categories. There are 13,320 videos in this dataset\n",
        "across the actions. The videos have a diversified variation of background, scale,\n",
        "pose, occlusion, and illumination conditions. The action categories are grouped\n",
        "into 25, which share similar variations such as the background, pose, scale,\n",
        "viewpoint, illumination and so on.\n",
        "\n",
        "##YouTube-8M\n",
        "is used for video classification.\n",
        "\n",
        "The dataset contains video URLs with labels and visual features.\n",
        "* Number of video URLs: 7 million\n",
        "* Hours of video clips: 450,000\n",
        "* Number of class labels: 4,716\n",
        "* Average number of labels per video: 3.4\n",
        "\n",
        "##Other datasets\n",
        "* **Sports-1M (Sports - 1 Million)**: Has 1,133,158 videos with 487 classes.\n",
        "The annotations are done automatically. The dataset can be downloaded\n",
        "from: http://cs.stanford.edu/people/karpathy/deepvideo/.\n",
        "* **UCF-11 (University of Central Florida - 11 actions)**: Has 1,600 videos\n",
        "with 11 actions. The videos have 29.97 fps (frames per second). The dataset\n",
        "can be downloaded along with UCF101.\n",
        "* **HMDB-51 (Human Motion DataBase - 51 actions)**: Has 5,100 videos\n",
        "with 51 actions. The dataset link is: http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database.\n",
        "* **Hollywood2**: Has 1,707 videos with 12 actions. The dataset link is: http://www.di.ens.fr/~laptev/actions/hollywood2."
      ]
    },
    {
      "metadata": {
        "id": "huQnumWG_l8u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following helper function is used to *load a video and split it into frames for further processing.*\n",
        "\n",
        "Note: this may take a lot of harddisk space."
      ]
    },
    {
      "metadata": {
        "id": "4z-qa-XjAE4w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "video_path = 'your_working_directory/your_video'\n",
        "video_handle = cv2.VideoCapture(video_path)\n",
        "frame_no = 0\n",
        "\n",
        "while True:\n",
        "  eof, frame = video_handle.read()\n",
        "  if not eof:\n",
        "    break\n",
        "  cv2.imwrite('frame{}.jpg'.format(frame_no), frame)\n",
        "  frame_no += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QHkYOpjoBYFG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Approaches for Classifying Videos\n",
        "Videos have to be classified for several applications. Since the video is a lot of\n",
        "data, training and inference computations must also be accounted for. All video\n",
        "classification approaches are inspired by image classification algorithms. The\n",
        "standard architectures such as VGG, Inception, and so on are used for feature\n",
        "computation at a frame level and then processed further.\n",
        "\n",
        "The following approaches can be used for video classification:\n",
        "* Extract the frames and use the models for classification on a frame basis.\n",
        "* Extract the image features and the features can be used train an RNN.\n",
        "* Train a **3D convolution** network on the whole video. 3D convolution is an extension of 2D convolution.\n",
        "* Use the optical flow of the video to further improve the accuracy. Optical\n",
        "flow is the pattern of movement of objects."
      ]
    },
    {
      "metadata": {
        "id": "H7o7BNsQCDy_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Fusing parallel CNN for video classification\n",
        "Frame-wise, the prediction of a video may not yield good results due to the\n",
        "downsampling of images, which loses fine details. Using a high-resolution CNN\n",
        "will increase the inference time.\n",
        "\n",
        "Karpathy proposed fusing two streams that are run in parallel for video classification. However, there are two problems with doing frame-wise predictions:\n",
        "* Predictions may take a long time because of the larger CNN architecture\n",
        "* Independent predictions lose the information along the temporal dimension\n",
        "\n",
        "The architecture can be simplified with fewer parameters with two smaller\n",
        "encoders running in parallel. The video is passed simultaneously through two\n",
        "CNN encoders. One encoder takes a low resolution and processes high\n",
        "resolution. The encoder has alternating convolution, normalization, and pooling\n",
        "layers. The final layer of the two encoders is connected through the fully\n",
        "connected layer. \n",
        "\n",
        "The other encoder is of the same size, but takes only the central crop,\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch9/parallel_CNN.PNG?raw=true)\n",
        "\n",
        "Parallel processing of frames makes the runtime faster by downsampling the\n",
        "video. The CNN architecture is halved regarding the parameter while\n",
        "maintaining the same accuracy. The two streams are called the **fovea** and\n",
        "**context** streams."
      ]
    },
    {
      "metadata": {
        "id": "MwuZQil8DmBc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After setting up the helper functions, we can create those two streams as shown below:"
      ]
    },
    {
      "metadata": {
        "id": "KDao6dOoDtFe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q_zVHJHNDwaw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1uNyFfiNDx3D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "no_classes = 10\n",
        "batch_size = 100\n",
        "total_batches = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EuKIFtbZD1Bc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_variable_summary(tf_variable, summary_name):\n",
        "  with tf.name_scope(summary_name + '_summary'):\n",
        "    mean = tf.reduce_mean(tf_variable)\n",
        "    tf.summary.scalar('Mean', mean)\n",
        "    with tf.name_scope('standard_deviation'):\n",
        "      standard_deviation = tf.sqrt(tf.reduce_mean(tf.square(tf_variable - mean)))\n",
        "        \n",
        "    tf.summary.scalar('StandardDeviation', standard_deviation)\n",
        "    tf.summary.scalar('Maximum', tf.reduce_max(tf_variable))\n",
        "    tf.summary.scalar('Minimum', tf.reduce_min(tf_variable))\n",
        "    tf.summary.histogram('Histogram', tf_variable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_dJ43CT6EDvg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convolution_layer(input_layer, \n",
        "                      filters, \n",
        "                      kernel_size=[3, 3],\n",
        "                      activation=tf.nn.relu):\n",
        "  layer = tf.layers.conv2d(inputs=input_layer,\n",
        "                           filters=filters,\n",
        "                           kernel_size=kernel_size,\n",
        "                           activation=activation)\n",
        "  \n",
        "  add_variable_summary(layer, 'convolution')\n",
        "  return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-FcYdX8SEPZ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pooling_layer(input_layer, pool_size=[2, 2], strides=2):\n",
        "  layer = tf.layers.max_pooling2d(inputs=input_layer,\n",
        "                                  pool_size=pool_size,\n",
        "                                  strides=strides)\n",
        "  add_variable_summary(layer, 'pooling')\n",
        "  return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "frAjSIhyEX5Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dense_layer(input_layer, units, activation=tf.nn.relu):\n",
        "  layer = tf.layers.dense(inputs=input_layer,\n",
        "                          units=units,\n",
        "                          activation=activation)\n",
        "  add_variable_summary(layer, 'dense')\n",
        "  return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pYqKmaTVEl5K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model(input_):\n",
        "  input_reshape = tf.reshape(input_,\n",
        "                             [-1, 28, 28, 1],\n",
        "                             name='input_reshape')\n",
        "  convolution_layer_1 = convolution_layer(input_reshape, 64)\n",
        "  pooling_layer_1 = pooling_layer(convolution_layer_1)\n",
        "  convolution_layer_2 = convolution_layer(pooling_layer_1, 128)\n",
        "  pooling_layer_2 = pooling_layer(convolution_layer_2)\n",
        "  flattened_pool = tf.reshape(pooling_layer_2, \n",
        "                              [-1, 5 * 5 * 128],\n",
        "                              name='flattened_pool')\n",
        "  return flattened_pool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XZLGFu31FVp1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can build the two CNN and combine them together:"
      ]
    },
    {
      "metadata": {
        "id": "vqVOPAmYEniu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "high_resolution_input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
        "low_resolution_input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
        "y_input = tf.placeholder(tf.float32, shape=[None, no_classes])\n",
        "\n",
        "high_resolution_cnn = get_model(high_resolution_input)\n",
        "low_resolution_cnn = get_model(low_resolution_input)\n",
        "dense_layer_1 = tf.concat([high_resolution_cnn, low_resolution_cnn], 1)\n",
        "dense_layer_bottleneck = dense_layer(dense_layer_1, 1024)\n",
        "logits = dense_layer(dense_layer_bottleneck, no_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BGoCxZC9GO00",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The reset of the code is to define the loss function and the optimization process for training and testing:"
      ]
    },
    {
      "metadata": {
        "id": "syPfmt8JGVgX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('loss'):\n",
        "  softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_input,\n",
        "                                                                     logits=logits)\n",
        "  loss_operation = tf.reduce_mean(softmax_cross_entropy, name='loss')\n",
        "  tf.summary.scalar('loss', loss_operation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EAQGRnBgGmj0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('optimiser'):\n",
        "  optimiser = tf.train.AdamOptimizer().minimize(loss_operation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eug4246TGpxp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('accuracy'):\n",
        "  with tf.name_scope('correct_prediction'):\n",
        "    predictions = tf.argmax(logits, 1)\n",
        "    correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1))\n",
        "  with tf.name_scope('accuracy'):\n",
        "    accuracy_operation = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "  tf.summary.scalar('accuracy', accuracy_operation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lh1aXB1kHCn8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "session = tf.Session()\n",
        "session.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RWTG1c7CHH3s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "merged_summary_operation = tf.summary.merge_all()\n",
        "train_summary_writer = tf.summary.FileWriter('./tmp/ch9/train', session.graph)\n",
        "test_summary_writer = tf.summary.FileWriter('./tmp/ch9/test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0qaQTJ2HHY40",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WNZaMw0pHab0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for batch_no in range(total_batches):\n",
        "  mnist_batch = mnist_data.train.next_batch(batch_size)\n",
        "  train_images, train_labels = mnist_batch[0], mnist_batch[1]\n",
        "  _, merged_summary = session.run([optimiser, merged_summary_operation],\n",
        "                                  feed_dict={high_resolution_input: train_images,\n",
        "                                             low_resolution_input: train_images,\n",
        "                                             y_input: train_labels\n",
        "                                            }\n",
        "                                 )\n",
        "  train_summary_writer.add_summary(merged_summary, batch_no)\n",
        "  if batch_no % 10 == 0:\n",
        "    merged_summary, _ = session.run([merged_summary_operation, accuracy_operation], \n",
        "                                    feed_dict={high_resolution_input: test_images,\n",
        "                                               low_resolution_input: test_images,\n",
        "                                               y_input: test_labels\n",
        "                                              }\n",
        "                                   )\n",
        "    test_summary_writer.add_summary(merged_summary, batch_no)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1cY4QBP9H8_j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The frames for processing across temporal dimensions are as shown below:\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch9/parallel_CNN_temporal.PNG?raw=true)\n",
        "\n",
        "Instead of going through fixed size clips, the video can be seen at different times.\n",
        "\n",
        "Three ways of connecting the temporal information are presented in the\n",
        "preceding image. Late fusion requires a longer time frame while early fusion\n",
        "sees a few frames together. Slow fusion combines both late and early fusion to\n",
        "give good results. The model was trained on the Sports1M dataset, which has 487\n",
        "classes and achieved an accuracy of 50%. The same model, when applied to\n",
        "UCF101, achieves an accuracy of 60%."
      ]
    },
    {
      "metadata": {
        "id": "SKOCDk0lImjD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Classifying videos over long periods\n",
        "The fusing method works well only for short videos.\n",
        "\n",
        "Ng proprosed two methods for classifying longer videos:\n",
        "* The first approach is to pool the convolutional features temporally. Max-\n",
        "pooling is used as a feature aggregation method.\n",
        "* The second approach has an LSTM connecting the convolutional features\n",
        "that handle the variable length of the video.\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch9/ng_method.PNG?raw=true)\n",
        "\n",
        "The CNN features can be extracted and fed to a small LSTM network:"
      ]
    },
    {
      "metadata": {
        "id": "s6M55wMqJUda",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_shape = [500,500]\n",
        "no_classes = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MAOI4tLnJY-P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net = tf.keras.models.Sequential()\n",
        "\n",
        "net.add(tf.keras.layers.LSTM(2048,\n",
        "                             return_sequences=False,\n",
        "                             input_shape=input_shape,\n",
        "                             dropout=0.5))\n",
        "net.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "net.add(tf.keras.layers.Dropout(0.5))\n",
        "net.add(tf.keras.layers.Dense(no_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6v5Vah51JwBi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adding LSTM for feature pooling instead provides better performance. The\n",
        "features are pooled in various ways, as shown in the following image:\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch9/feature_pooling.PNG?raw=true)\n",
        "\n",
        "the convolutional features can be aggregated in several\n",
        "different ways. The pooling is done after the fully connected layer before it. This\n",
        "method achieved an accuracy of 73.1% and 88.6% in the Sports1M dataset and\n",
        "UCF101 datasets respectively."
      ]
    },
    {
      "metadata": {
        "id": "yO12RfoGKXkb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The LSTM approach is shown below:\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch9/feature_pooling.PNG?raw=true)\n",
        "\n",
        "The computations are high for this model because several LSTM's are used."
      ]
    },
    {
      "metadata": {
        "id": "9mr7GyIeKr0X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Streaming two CNN's for action recognition\n",
        "The motion of objects in videos has very good information about the actions\n",
        "performed in the video. The motion of objects can be quantified by optical flow.\n",
        "\n",
        "Zisserman proposed a method for action recognition that uses two streams from images and optical flow.\n",
        "\n",
        "Optical flow measures the motion by quantifying the relative movement between\n",
        "the observer and scene. The optical flow can be obtained by running the built-in function in OpenCV:\n",
        "\n",
        "`p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)`\n",
        "\n",
        "One stream takes an individual frame and predicts actions using a regular CNN.\n",
        "The other stream takes multiple frames and computes the optical flow. The\n",
        "optical flow is passed through a CNN for a prediction.\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch9/stream_action_recognition.PNG?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "aPn__xEkLpfe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Using 3D convolution for temporal learning\n",
        "A video can be classified with 3D convolution. 3D convolution operation takes a\n",
        "volume as input and outputs the same, whereas a 2D convolution can take a 2D\n",
        "or volume output and outputs a 2D image.\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch9/3d_conv.PNG?raw=true)\n",
        "\n",
        "The first two images belong to 2D convolution. The output is always an image.\n",
        "3D convolution, meanwhile, outputs a volume. The difference is a convolution\n",
        "operation in 3 directions with the kernel.\n",
        "\n",
        "An example of 3D convoluational neural network is shown below:"
      ]
    },
    {
      "metadata": {
        "id": "CJqZTHRzM_lU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "input_shape = (227, 227, 200, 3)\n",
        "no_classes = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cZBkRbBfNBJ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net = tf.keras.models.Sequential()\n",
        "\n",
        "net.add(tf.keras.layers.Conv3D(32,\n",
        "                               kernel_size=(3,3,3),\n",
        "                               input_shape=input_shape))\n",
        "net.add(tf.keras.layers.Activation('relu'))\n",
        "net.add(tf.keras.layers.Conv3D(32, (3, 3, 3)))\n",
        "net.add(tf.keras.layers.Activation('softmax'))\n",
        "net.add(tf.keras.layers.MaxPooling3D())\n",
        "net.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "net.add(tf.keras.layers.Conv3D(64, (3, 3, 3)))\n",
        "net.add(tf.keras.layers.Activation('relu'))\n",
        "net.add(tf.keras.layers.Conv3D(64, (3, 3, 3)))\n",
        "net.add(tf.keras.layers.Activation('softmax'))\n",
        "net.add(tf.keras.layers.MaxPool3D())\n",
        "net.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "net.add(tf.keras.layers.Flatten())\n",
        "net.add(tf.keras.layers.Dense(512, activation='sigmoid'))\n",
        "net.add(tf.keras.layers.Dropout(0.5))\n",
        "net.add(tf.keras.layers.Dense(no_classes, activation='softmax'))\n",
        "net.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "            optimizer=tf.keras.optimizers.Adam(), \n",
        "            metrics=['accuracy'])\n",
        "net.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uy5i512ZNsVr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3D convolution needs a lot of computing power. 3D convolution\n",
        "achieves an accuracy of 90.2% on the Sports1M dataset."
      ]
    },
    {
      "metadata": {
        "id": "TLfzW4wTNuen",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Using trajectory for classification\n",
        "Wang used the trajectory of parts of bodies to classify the actions performed. This work combines handcrafted and deep learned features for final predictions.\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch9/trajectory.PNG?raw=true)\n",
        "\n",
        "The handcrafted features are **Fisher vector** and the features are from CNN. The\n",
        "following image demonstrates the extraction of the trajectories and features\n",
        "maps:\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch9/trajectory_flowchart.PNG?raw=true)\n",
        "\n",
        "Both the trajectories and features maps are combined temporally to form the\n",
        "final predictions over the temporal snippet."
      ]
    },
    {
      "metadata": {
        "id": "hQD2A0NLOl1v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Multi-modal fusion\n",
        "Yang proposed a multi-modal fusion, with 4 models, for video classification. The four models are 3D convolution features, 2D optical flow, 3D\n",
        "optical flow, and 2D convolution features. The data flowchart is shown below:\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch9/multi-modal.PNG?raw=true)\n",
        "\n",
        "A Convlet is the small convolutional output from\n",
        "a single kernel. The learning of spatial weights in the convolution layer by\n",
        "convlet is shown in the following image:\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch9/multi-modal%20feature.PNG?raw=true)\n",
        "\n",
        "A spatial weight indicates how discriminative or important a local spatial region\n",
        "is in a convolutional layer. The following image is an illustration of fusing multilayer representation, done at various layers of convolutional and fully connected layers:\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch9/multi-modal%20detail.PNG?raw=true)\n",
        "\n",
        "The boosting mechanism is used to combine the predictions. **Boosting** is a\n",
        "mechanism that can combine several model prediction into a final prediction."
      ]
    },
    {
      "metadata": {
        "id": "-dcr_-WzQJWF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Attending regions for classification\n",
        "An attention mechanism can be used for classification. Attention mechanisms\n",
        "replicate the human behaviour of focusing on regions for recognition activities.\n",
        "Attention mechanisms give more weight to certain regions than others. The\n",
        "method of weight is learned from the data while training.\n",
        "\n",
        "There are two types of attention mechanisms:\n",
        "* **Soft attention**: Deterministic in character, this can hence be learned by\n",
        "back-propagation.\n",
        "* **Hard attention**: Stochastic in character, this needs complicated\n",
        "mechanisms to learn. It is also expensive because of the requirement of\n",
        "sampling data.\n",
        "\n",
        "A visualization of soft attention:\n",
        "\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch9/soft_attention.PNG?raw=true)\n",
        "\n",
        "The CNN features are computed and weighted according to the attention. The attention or weights given to certain areas can be used for visualization. LSTM were used to take the convolution features. The LSTM predicts the\n",
        "regions by using attention on following frames,\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch9/attention_lstm.PNG?raw=true)\n",
        "\n",
        "Each stack of LSTM predicts location and labels. Every stack has three LSTM.\n",
        "The input to the LSTM stack is a convolution feature cube and location. The\n",
        "location probabilities are the attention weights. The use of attention gives an\n",
        "improvement in accuracy as well as a method to visualize the predictions."
      ]
    }
  ]
}