{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch6_similarity_learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lblogan14/deep_learning_for_computer_vision/blob/master/ch6_similarity_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "MX9GSasoGlPp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9ce4b043-60d3-4dd1-b282-5bcedb2bfc0f"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-TIFsVd-Gn9g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13645751-c759-4baa-94b5-46237815723e"
      },
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My' 'Drive/Colab' 'Notebooks/Deep_Learning_for_Computer_Vision/"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Deep_Learning_for_Computer_Vision\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2-gwzNd6-g4k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UUh60k9h6wAi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Similarity Learning Algorithms\n",
        "**Similarity learning** is the process of training a metric to compute the similarity between two entities. A metric can be Euclidean or cosine or other custom distance function. Entities can be any data, such as images, videos, texts, or tables. A vector representation of the image is requried when to compute a metric."
      ]
    },
    {
      "metadata": {
        "id": "uOC0UCXx75rz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Siamese Network\n",
        "is a neural network model where the network is trained to distinguish between two inputs.\n",
        "\n",
        "A Siamese network can\n",
        "train a CNN to produced an embedding by two encoders. Each encoder is fed\n",
        "with one of the images in either a positive or a negative pair. A Siamese network\n",
        "requires less data than the other deep learning algorithms. Siamese networks\n",
        "were originally introduced for comparing signatures.\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch6/siamese.JPG?raw=true)\n",
        "\n",
        "Siamese networks can also be used for one-shot learning. **One-shot learning** is to learning with only one example. In this case, an image can be\n",
        "shown and it can tell whether they are similar. For most of the similarity learning\n",
        "tasks, a pair of positive and negative pairs are required to train. Such datasets can\n",
        "be formed with any dataset that is available for classification tasks, assuming\n",
        "that they are Euclidean distances. Here, the main objective of the encoders is to differentiate one from another."
      ]
    },
    {
      "metadata": {
        "id": "z8fkPSLI9W7L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Contrastive Loss\n",
        "differentiates images by similarity. \n",
        "\n",
        "The feature or latent layer is\n",
        "compared using a similarity metric and trained with the target for a similarity\n",
        "score. In the case of a positive pair, the target would be 0, as both inputs are the\n",
        "same. For negative pairs, the distance between the pair of latent is a maximum of\n",
        "0 in the case of cosine distance or regularised Euclidean distance.\n",
        "\n",
        "The `contrastive_loss` is defined as below:"
      ]
    },
    {
      "metadata": {
        "id": "TrP82hRz9uQL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def contrastive_loss(model_1, model_2, label, margin=0.1):\n",
        "  distance = tf.redunce_sum(tf.square(model_1 - model_2), 1)\n",
        "  loss = label * tf.square(tf.maximum(0., margin-tf.sqrt(distance))) + (1 - label) * distance\n",
        "  loss = 0.5 * tf.reduce_mean(loss)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wWrb7Z60_ZBg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Train a Siamese Network\n",
        "We need two models.\n",
        "\n",
        "Set up the layers and datasets first,"
      ]
    },
    {
      "metadata": {
        "id": "rubKA2Gw_1hG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "792b402e-feb0-4895-cffc-248a6b814813"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "\n",
        "input_size = 784\n",
        "no_classes = 10\n",
        "batch_size = 100\n",
        "total_batches = 300"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mPu_RPQtAEyI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_variable_summary(tf_variable, summary_name):\n",
        "  with tf.name_scope(summary_name + '_summary'):\n",
        "    mean = tf.reduce_mean(tf_variable)\n",
        "    tf.summary.scalar('Mean', mean)\n",
        "    with tf.name_scope('standard_deviation'):\n",
        "      standard_deviation = tf.sqrt(tf.reduce_mean(tf.square(tf_variable - mean)))\n",
        "    tf.summary.scalar('StandardDeviation', standard_deviation)\n",
        "    tf.summary.scalar('Maximum', tf.reduce_max(tf_variable))\n",
        "    tf.summary.scalar('Minimum', tf.reduce_min(tf_variable))\n",
        "    tf.summary.histogram('Histogram', tf_variable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eWcBEdH0B9jl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convolution_layer(input_layer, filters, kernel_size=[3,3],\n",
        "                      activation=tf.nn.relu):\n",
        "  layer = tf.layers.conv2d(inputs=input_layer,\n",
        "                           filters=filters,\n",
        "                           kernel_size=kernel_size,\n",
        "                           activation=activation)\n",
        "  add_variable_summary(layer, 'convolution')\n",
        "  return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UqEoyPmtCP7O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pooling_layer(input_layer, pool_size=[2,2], strides=2):\n",
        "  layer = tf.layers.max_pooling2d(inputs=input_layer,\n",
        "                                  pool_size=pool_size,\n",
        "                                  strides=strides)\n",
        "  add_variable_summary(layer, 'pooling')\n",
        "  return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fyfim4oECcwe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dense_layer(input_layer, units, activation=tf.nn.relu):\n",
        "  layer = tf.layers.dense(inputs=input_layer,\n",
        "                          units=units,\n",
        "                          activation=activation)\n",
        "  add_variable_summary(layer, 'dense')\n",
        "  return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aLYhLS2_CshI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use the building blocks above to build a simple CNN,"
      ]
    },
    {
      "metadata": {
        "id": "AyjG4BFkCx6k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model(input_):\n",
        "  input_reshape = tf.reshape(input_, [-1,28,28,1], name='input_reshape')\n",
        "  convolution_layer_1 = convolution_layer(input_reshape, 64)\n",
        "  pooling_layer_1 = pooling_layer(convolution_layer_1)\n",
        "  convolution_layer_2 = convolution_layer(pooling_layer_1, 128)\n",
        "  pooling_layer_2 = pooling_layer(convolution_layer_2)\n",
        "  flattened_pool = tf.reshape(pooling_layer_2, [-1, 5*5*128], name='flattened_pool')\n",
        "  dense_layer_bottleneck = dense_layer(flattened_pool, 1024)\n",
        "  return dense_layer_bottleneck"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DBqfpVvkDiE4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model defined will be used twice to define the encoders necessary for\n",
        "Siamese networks.\n",
        "\n",
        "Next, placeholders for both the models are defined. For every\n",
        "pair, the similarity of the inputs is also fed as input. The models defined are the\n",
        "same. The models can also be defined so that the weights are shared."
      ]
    },
    {
      "metadata": {
        "id": "LiG3xjzXEPuI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
        "bottom_input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
        "y_input = tf.placeholder(tf.float32, shape=[None, no_classes])\n",
        "\n",
        "top_bottleneck = get_model(top_input)\n",
        "bottom_bottleneck = get_model(bottom_input)\n",
        "# Concatenate models for similarity learning\n",
        "dense_layer_bottleneck = tf.concat([top_bottleneck, bottom_bottleneck], 1)\n",
        "\n",
        "dropout_bool = tf.placeholder(tf.bool)\n",
        "dropout_layer = tf.layers.dropout(inputs=dense_layer_bottleneck,\n",
        "                                  rate=0.4,\n",
        "                                  training=dropout_bool)\n",
        "logits = dense_layer(dropout_layer, no_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mRPtn-_DFK6K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('loss'):\n",
        "  softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_input,\n",
        "                                                                     logits=logits)\n",
        "  loss_operation = tf.reduce_mean(softmax_cross_entropy, name='loss')\n",
        "  tf.summary.scalar('loss', loss_operation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KNs6sFuAFbey",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('optimizer'):\n",
        "  optimizer = tf.train.AdamOptimizer().minimize(loss_operation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ZMup8GgFhKB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope('accuracy'):\n",
        "  with tf.name_scope('correct_prediction'):\n",
        "    predictions = tf.argmax(logits, 1)\n",
        "    correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1))\n",
        "  with tf.name_scope('accuracy'):\n",
        "    accuracy_operation = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "  tf.summary.scalar('accuracy', accuracy_operation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NVXkKTdkGA0g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "session = tf.Session()\n",
        "session.run(tf.global_variables_initializer())\n",
        "\n",
        "merged_summary_operation = tf.summary.merge_all()\n",
        "train_summary_writer = tf.summary.FileWriter('./tmp/ch6/train', session.graph)\n",
        "test_summary_writer = tf.summary.FileWriter('./tmp/ch6/test')\n",
        "\n",
        "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Qtgf0M9G5bQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can train the network and vusialize the result with TensorBoard."
      ]
    },
    {
      "metadata": {
        "id": "IuNEVK_9G_AM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for batch_no in range(total_batches):\n",
        "  mnist_batch = mnist_data.train.next_batch(batch_size)\n",
        "  train_images, train_labels = mnist_batch[0], mnist_batch[1]\n",
        "  _, merged_summary = session.run([optimizer, merged_summary_operation],\n",
        "                                  feed_dict={top_input:train_images,\n",
        "                                             bottom_input:train_images,\n",
        "                                             y_input:train_labels,\n",
        "                                             dropout_bool:True}\n",
        "                                 )\n",
        "  train_summary_writer.add_summary(merged_summary, batch_no)\n",
        "  if batch_no % 10 == 0:\n",
        "    merged_summary, _ = session.run([merged_summary_operation, accuracy_operation],\n",
        "                                    feed_dict={top_input:test_images,\n",
        "                                               bottom_input:test_images,\n",
        "                                               y_input:test_labels,\n",
        "                                               dropout_bool:False}\n",
        "                                   )\n",
        "    test_summary_writer.add_summary(merged_summary, batch_no)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WmjFV4Z4IqEf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Two encoders are defined, and\n",
        "the latent space is concatenated to form the loss of training. The top and bottom\n",
        "models are fed with data separately."
      ]
    },
    {
      "metadata": {
        "id": "Ol_EQ0NyIKcK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####TensorBoard\n"
      ]
    },
    {
      "metadata": {
        "id": "zmEaHOjpIN-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "8d7918aa-dcc8-4996-8e5d-804e63599322"
      },
      "cell_type": "code",
      "source": [
        "# install ngrok\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-18 16:42:05--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.21.103.149, 52.203.66.95, 52.207.111.186, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.21.103.149|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]   5.11M  17.7MB/s    in 0.3s    \n",
            "\n",
            "2018-12-18 16:42:05 (17.7 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [5363700/5363700]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H36DMdZEIQad",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run TensorBoard,\n",
        "# locating the summary file\n",
        "# training summary -> ./tmp/train\n",
        "# testing summary -> ./tmp/test\n",
        "LOG_DIR = './tmp/ch6'\n",
        "get_ipython().system_raw(\n",
        "      'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format(LOG_DIR))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hdd064aDISV9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run ngrok\n",
        "# run ngrok to tunnel TensorBoard port 6006 to the outside world\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pNhovMUzIUVh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1fd4ed8-5973-4c64-868e-db475df9c4d7"
      },
      "cell_type": "code",
      "source": [
        "# Get URL\n",
        "# access the colab TensorBoard web page\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://ae1404b5.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ik1dIBJZIy8M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##FaceNet\n",
        "solves the face verification problem and learns one deep CNN then transforms a face image into an embedding.\n",
        "\n",
        "The FaceNet architecture is shown below:\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch6/facenet.JPG?raw=true)\n",
        "\n",
        "FaceNet takes a batch of face images and trains them. In that batch, there will be\n",
        "a few positive pairs. While computing the loss, the positive pairs and closest few\n",
        "negative pairs are considered. Mining selective pairs enable smooth training. If\n",
        "all the negatives are pushed away all the time, the training is not stable.\n",
        "Comparing three data points is called triplet loss. The images are considered\n",
        "with a positive and negative match while computing the loss. The negatives are\n",
        "pushed only by a certain margin."
      ]
    },
    {
      "metadata": {
        "id": "cd_NvgXRJ1gP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Triplet Loss\n",
        "learns the score vectors for the images. The score vectors of face\n",
        "descriptors can be used to verify the faces in Euclidean space. The triplet loss is\n",
        "similar to metric learning in the sense of learning a projection so that the inputs\n",
        "can be distinguished. These projections or descriptors or score vectors are a\n",
        "compact representation, hence can be considered as a dimensionality reduction\n",
        "technique.\n",
        "\n",
        "A **triplet** consists of an *anchor*, and *positive* and *negative faces*. An\n",
        "*anchor* can be any face, and *positive faces* are the images of the same person.\n",
        "The *negative image* may come from another person.\n",
        "\n",
        "There will be a lot of negative faces for a given anchor. By selecting negatives that are\n",
        "currently closer to the anchor, its harder for the encoder to distinguish the faces,\n",
        "thereby making it learn better. This process is termed as **hard negative mining**.\n",
        "The closer negatives can be obtained with a threshold in Euclidean space.\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch6/triplet.JPG?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "CKX4kI-TKzmn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `triplet_loss` functions can be defined as:"
      ]
    },
    {
      "metadata": {
        "id": "Jrq0-M5uK4LC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def triplet_loss(anchor_face, positive_face, negative_face, margin):\n",
        "  def get_distance(x, y):\n",
        "    return tf.reduce_sum(tf.square(tf.subtract(x, y)), 1)\n",
        "  \n",
        "  positive_distance = get_distance(anchor_face, positive_face)\n",
        "  negative_distance = get_distance(anchor_face, negative_face)\n",
        "  total_distance = tf.add(tf.subtract(positive_distance, negative_distance), margin)\n",
        "  return tf.reduce_mean(tf.maximum(total_distance, 0.0), 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tHbowRe1LKiR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Every point has to be compared with\n",
        "others to get the proper anchor and positive pairs. The mining of the triplets is\n",
        "shown below:"
      ]
    },
    {
      "metadata": {
        "id": "YZQzUROLLPH6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-CHU0NFRLUQA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mine_triplets(anchor, targets, negative_samples):\n",
        "  distances = cdist(anchor, targets, 'cosine')\n",
        "  distances = cdist(anchor, targets, 'cosine').tolist()\n",
        "  QnQ_duplicated = [\n",
        "      [target_index for target_index, dist in enumerate(QnQ_dist) if dist == QnQ_dist[query_index]]\n",
        "    for query_index, QnQ_dist in enumerate(distances)]\n",
        "  for i, QnT_dist in enumerate(QnT_dists):\n",
        "    for j in QnQ_duplicated[i]:\n",
        "      QnT_dist.itemset(j, np.inf)\n",
        "\n",
        "  QnT_dists_topk = QnT_dists.argsort(axis=1)[:, :negative_samples]\n",
        "  top_k_index = np.array([np.insert(QnT_dist, 0, i) for i, QnT_dist in enumerate(QnT_dists_topk)])\n",
        "  return top_k_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9hRiVaIYLg7Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The FaceNet model is a state of the art method in\n",
        "training similarity models for faces."
      ]
    },
    {
      "metadata": {
        "id": "btESv7JPLj41",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##DeepNet Model\n",
        "is used to leran the embedding of faces for face verification tasks. It improves on the method of FaceNet by taking multiple crops of the same face and passing through several encoders to get a better embedding. This has achieved a better accuracy than FaceNet but takes more time for processing.\n",
        "\n",
        "The face crops\n",
        "are made in the same regions and passed through its respective encoders. Then\n",
        "all the layers are concatenated for training against the triplet loss.\n",
        "\n",
        "##DeepRank\n",
        "is used to rank images based on similarity. \n",
        "\n",
        "Images are passed through different models:\n",
        "\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch6/deeprank.JPG?raw=true)\n",
        "\n",
        "The triplet loss is computed and backpropagation is done here as well.\n",
        "\n",
        "Then the image can be converted into a linear embedding for ranking. The DeepRank architecture is shown below:\n",
        "![alt text](https://github.com/lblogan14/deep_learning_for_computer_vision/blob/master/notes_images/ch6/deeprank2.JPG?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "6iJ-FnHxOCBu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Human Face Analysis\n",
        "* **Face detection**: Finding the bounding box of location of faces\n",
        "* **Facial landmark detection**: Finding the spatial points of facial features\n",
        "such as nose, mouth and so on\n",
        "* **Face alignment**: Transforming the face into a frontal face for further\n",
        "analysis\n",
        "* **Attribute recognition**: Finding attributes such as gender, smiling and so on\n",
        "* **Emotion analysis**: Analysing the emotions of persons\n",
        "* **Face verification**: Finding whether two images belong to the same person\n",
        "* **Face recognition**: Finding an identity for the face\n",
        "* **Face clustering**: Grouping the faces of the same person together"
      ]
    },
    {
      "metadata": {
        "id": "_pWHLAL1PFxb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Face Landmarks and Attributes\n",
        "**Face landmarks** are the spatial points in a human face. The spatial points\n",
        "correspond to locations of various facial features such as eyes, eyebrows, nose,\n",
        "mouth, and chin. The number of points may vary from 5 to 78 depending on the\n",
        "annotation. Face landmarks are also referred to as **fiducial-points**, **facial keypoints**, or **face pose**.\n",
        "\n",
        "Applications of face landmarks:\n",
        "* Alignment of faces for better face verification or face recognition\n",
        "* To track faces in a video\n",
        "* Facial expressions or emotions can be measured\n",
        "* Helpful for diagnosis of medical conditions"
      ]
    },
    {
      "metadata": {
        "id": "lCefAy1mPc6f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Learn the facial keypoints"
      ]
    },
    {
      "metadata": {
        "id": "DNBqwCF1Qpt5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_size = 40\n",
        "no_landmark = 10\n",
        "no_gender_classes = 2\n",
        "no_smile_classes = 2\n",
        "no_glasses_classes = 2\n",
        "no_headpose_classes = 5\n",
        "batch_size = 100\n",
        "total_batches = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "arBWdnzOSp-J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "muL_fFtsRJF1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Placeholders for various inputs:"
      ]
    },
    {
      "metadata": {
        "id": "hnMwlbmzQqeX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_input = tf.placeholder(tf.float32, shape=[None, image_size, image_size])\n",
        "\n",
        "landmark_input = tf.placeholder(tf.float32, shape=[None, no_landmark])\n",
        "gender_input = tf.placeholder(tf.float32, shape=[None, no_gender_classes])\n",
        "smile_input = tf.placeholder(tf.float32, shape=[None, no_smile_classes])\n",
        "glasses_input = tf.placeholder(tf.float32, shape=[None, no_glasses_classes])\n",
        "headpose_input = tf.placeholder(tf.float32, shape=[None, no_headpose_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NXbsXILkRM7K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "CNN model construction:"
      ]
    },
    {
      "metadata": {
        "id": "fjgitg96Qtfr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_input_reshape = tf.reshape(image_input, [-1, image_size, image_size, 1],\n",
        "                             name='input_reshape')\n",
        "\n",
        "convolution_layer_1 = convolution_layer(image_input_reshape, 16)\n",
        "pooling_layer_1 = pooling_layer(convolution_layer_1)\n",
        "convolution_layer_2 = convolution_layer(pooling_layer_1, 48)\n",
        "pooling_layer_2 = pooling_layer(convolution_layer_2)\n",
        "convolution_layer_3 = convolution_layer(pooling_layer_2, 64)\n",
        "pooling_layer_3 = pooling_layer(convolution_layer_3)\n",
        "convolution_layer_4 = convolution_layer(pooling_layer_3, 64)\n",
        "flattened_pool = tf.reshape(convolution_layer_4, [-1, 5 * 5 * 64],\n",
        "                            name='flattened_pool')\n",
        "dense_layer_bottleneck = dense_layer(flattened_pool, 1024)\n",
        "dropout_bool = tf.placeholder(tf.bool)\n",
        "dropout_layer = tf.layers.dropout(\n",
        "        inputs=dense_layer_bottleneck,\n",
        "        rate=0.4,\n",
        "        training=dropout_bool\n",
        "    )\n",
        "landmark_logits = dense_layer(dropout_layer, 10)\n",
        "smile_logits = dense_layer(dropout_layer, 2)\n",
        "glass_logits = dense_layer(dropout_layer, 2)\n",
        "gender_logits = dense_layer(dropout_layer, 2)\n",
        "headpose_logits = dense_layer(dropout_layer, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ju3kwPylRSBb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The loss is computed individually for all the facial features,"
      ]
    },
    {
      "metadata": {
        "id": "QBTc_GuoQwss",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "landmark_loss = 0.5 * tf.reduce_mean(\n",
        "    tf.square(landmark_input, landmark_logits))\n",
        "\n",
        "gender_loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(\n",
        "        labels=gender_input, logits=gender_logits))\n",
        "\n",
        "smile_loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(\n",
        "        labels=smile_input, logits=smile_logits))\n",
        "\n",
        "glass_loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(\n",
        "        labels=glasses_input, logits=glass_logits))\n",
        "\n",
        "headpose_loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(\n",
        "        labels=headpose_input, logits=headpose_logits))\n",
        "\n",
        "loss_operation = landmark_loss + gender_loss + \\\n",
        "                 smile_loss + glass_loss + headpose_loss\n",
        "\n",
        "optimiser = tf.train.AdamOptimizer().minimize(loss_operation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TssHt5k0QzuU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Need to create the fiducial image data first\n",
        "# a module named fiducial_data needs to be created and imported \n",
        "session = tf.Session()\n",
        "session.run(tf.initialize_all_variables())\n",
        "fiducial_test_data = fiducial_data.test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VCGOkKfrRAUF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for batch_no in range(total_batches):\n",
        "    fiducial_data_batch = fiducial_data.train.next_batch(batch_size)\n",
        "    loss, _landmark_loss, _ = session.run(\n",
        "        [loss_operation, landmark_loss, optimiser],\n",
        "        feed_dict={\n",
        "            image_input: fiducial_data_batch.images,\n",
        "            landmark_input: fiducial_data_batch.landmarks,\n",
        "            gender_input: fiducial_data_batch.gender,\n",
        "            smile_input: fiducial_data_batch.smile,\n",
        "            glasses_input: fiducial_data_batch.glasses,\n",
        "            headpose_input: fiducial_data_batch.pose,\n",
        "            dropout_bool: True\n",
        "    })\n",
        "    if batch_no % 10 == 0:\n",
        "        loss, _landmark_loss, _ = session.run(\n",
        "            [loss_operation, landmark_loss],\n",
        "            feed_dict={\n",
        "                image_input: fiducial_test_data.images,\n",
        "                landmark_input: fiducial_test_data.landmarks,\n",
        "                gender_input: fiducial_test_data.gender,\n",
        "                smile_input: fiducial_test_data.smile,\n",
        "                glasses_input: fiducial_test_data.glasses,\n",
        "                headpose_input: fiducial_test_data.pose,\n",
        "                dropout_bool: False\n",
        "            })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zxNtZNCuRVyt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Face Recognition\n",
        "is the process of identifying a personage from a digital image or a video.\n",
        "\n",
        "###Compute the similarity between faces\n",
        "The faces have to be\n",
        "detected, followed by finding the fiducial points. The faces can be aligned with\n",
        "the fiducial points. The aligned face can be used for comparison."
      ]
    },
    {
      "metadata": {
        "id": "B5hQ5KjKR5F9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy import misc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import facenet\n",
        "print facenet\n",
        "from facenet import load_model, prewhiten\n",
        "import align.detect_face"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LzxtEVoqSm2p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bcRQbMunR-Og",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load and align the images:"
      ]
    },
    {
      "metadata": {
        "id": "p_8yzXGHR8la",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_and_align_data(image_paths, image_size=160, margin=44, gpu_memory_fraction=1.0):\n",
        "    minsize = 20\n",
        "    threshold = [0.6, 0.7, 0.7]\n",
        "    factor = 0.709\n",
        "\n",
        "    print('Creating networks and loading parameters')\n",
        "    with tf.Graph().as_default():\n",
        "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n",
        "        sess = tf.Session(config=tf.ConfigProto(\n",
        "            gpu_options=gpu_options, log_device_placement=False))\n",
        "        with sess.as_default():\n",
        "            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n",
        "\n",
        "    nrof_samples = len(image_paths)\n",
        "    img_list = [None] * nrof_samples\n",
        "    for i in range(nrof_samples):\n",
        "        img = misc.imread(os.path.expanduser(image_paths[i]), mode='RGB')\n",
        "        img_size = np.asarray(img.shape)[0:2]\n",
        "        bounding_boxes, _ = align.detect_face.detect_face(\n",
        "            img, minsize, pnet, rnet, onet, threshold, factor)\n",
        "        det = np.squeeze(bounding_boxes[0, 0:4])\n",
        "        bb = np.zeros(4, dtype=np.int32)\n",
        "        bb[0] = np.maximum(det[0] - margin / 2, 0)\n",
        "        bb[1] = np.maximum(det[1] - margin / 2, 0)\n",
        "        bb[2] = np.minimum(det[2] + margin / 2, img_size[1])\n",
        "        bb[3] = np.minimum(det[3] + margin / 2, img_size[0])\n",
        "        cropped = img[bb[1]:bb[3], bb[0]:bb[2], :]\n",
        "        aligned = misc.imresize(\n",
        "            cropped, (image_size, image_size), interp='bilinear')\n",
        "        prewhitened = prewhiten(aligned)\n",
        "        img_list[i] = prewhitened\n",
        "    images = np.stack(img_list)\n",
        "    return images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oOxtTTpHSOMY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Process the image paths to get the embeddings:"
      ]
    },
    {
      "metadata": {
        "id": "v9fcxsvlSQY4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_face_embeddings(image_paths, model=''):\n",
        "    images = load_and_align_data(image_paths)\n",
        "    with tf.Graph().as_default():\n",
        "        with tf.Session() as sess:\n",
        "            load_model(model)\n",
        "            images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
        "            embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
        "            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
        "            feed_dict = {images_placeholder: images,phase_train_placeholder: False}\n",
        "            emb = sess.run(embeddings, feed_dict=feed_dict)\n",
        "\n",
        "    return emb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H_IQmN6MSdZG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compute the distance between the embeddings:"
      ]
    },
    {
      "metadata": {
        "id": "kFU27061SgIX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_distance(embedding_1, embedding_2):\n",
        "    dist = np.sqrt(np.sum(np.square(np.subtract(embedding_1, embedding_2))))\n",
        "    return dist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H4BepIbDSiqJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "whih computes the Euclidean distance between the embeddings."
      ]
    },
    {
      "metadata": {
        "id": "rdvv7laZSthY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Find the optimum threshold\n",
        "Combining with the preceding functions, we can calculate the accuracy of this model:"
      ]
    },
    {
      "metadata": {
        "id": "s1iT5SKJTBQH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import argparse\n",
        "import os\n",
        "import re\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dd_REFvxTJvI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use the following function to obtain the images:"
      ]
    },
    {
      "metadata": {
        "id": "k_GLnqe9TNhf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_image_paths(image_directory):\n",
        "  image_names = sorted(os.listdir(image_directory))\n",
        "  image_paths = [os.path.join(image_directory, image_name) for image_name in image_names]\n",
        "  return image_paths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R7Ye9tJ1Tib_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The distances of the images are obtained when embeddings are passed:"
      ]
    },
    {
      "metadata": {
        "id": "gm2abf4zTlAz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_labels_distances(image_paths, embeddings):\n",
        "  target_labels, distances = [], []\n",
        "  for image_path_1, embedding_1 in zip(image_paths, embeddings):\n",
        "    for image_path_2, embedding_2 in zip(image_paths, embeddings):\n",
        "      if (re.sub(r'\\d+', '', image_path_1)).lower() == (re.sub(r'\\d+', '', image_path_2)).lower():\n",
        "        target_labels.append(1)\n",
        "      else:\n",
        "        target_labels.append(0)\n",
        "      distances.append(compute_distance(embedding_1, embedding_2))\n",
        "  return target_labels, distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-mIgFdRbUTRA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Print out the threshold and accuracy:"
      ]
    },
    {
      "metadata": {
        "id": "dOnUGM4vUdnO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_metrics(target_labels, distances):\n",
        "  accuracies = []\n",
        "  for threshold in range(50, 150, 1):\n",
        "    threshold = threshold/100\n",
        "    predicted_labels = [1 if dist <= threshold\n",
        "                          else 0 for dist in distances]\n",
        "    print('Threshold ', threshold)\n",
        "    print(classification_report(target_labels, predicted_labels))\n",
        "    accuracy = accuracy_score(target_labels, predicted_labels)\n",
        "    print('Accuracy: ', accuracy)\n",
        "    accuracies.append(accuracy)\n",
        "  print('Highest accuracy: ', max(accuracies))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SO9i2Ok4V03h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Execute the main codes:"
      ]
    },
    {
      "metadata": {
        "id": "MXk68Q18VgUP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_paths = get_image_paths(image_directory)\n",
        "embeddings = get_face_embeddings(image_paths)\n",
        "target_labels, distances = get_labels_distances(image_paths, embeddings)\n",
        "print_metrics(target_labels, distances)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jIWzN0NZWCnH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Face Clustering\n",
        "is the process of grouping images of the same person together.\n",
        "\n",
        "The embeddings of faces can be extracted, and a clustering\n",
        "algorithm such as K-means can be used to club the faces of the same person\n",
        "together. TensorFlow provides an API called `tf.contrib.learn.KmeansClustering` for\n",
        "the K-means algorithm."
      ]
    }
  ]
}